start!
compression ratio: 0.5 ::: pruner: randTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.302585           7.56          50.10
Final      200    0.051553   0.767854          88.05          98.98
Prune results:
             module   param  sparsity     size             shape  ...    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.306713     1728     (64, 3, 3, 3)  ...   -46.742149        0.805873            0.356814   1.392548e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.315294    36864    (64, 64, 3, 3)  ...    49.684040        0.798002            0.364637   2.941756e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.317234    73728   (128, 64, 3, 3)  ...   293.166901        0.796496            0.363902   5.872405e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.314575   147456  (128, 128, 3, 3)  ...  -513.308838        0.797132            0.361494   1.175419e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.316352   294912  (256, 128, 3, 3)  ...    78.829033        0.800595            0.365375   2.361051e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.317373   589824  (256, 256, 3, 3)  ...  1204.694092        0.797834            0.363172   4.705814e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.315513   589824  (256, 256, 3, 3)  ... -1353.743652        0.798674            0.364395   4.710770e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.316756  1179648  (512, 256, 3, 3)  ...  -192.390137        0.799088            0.363946   9.426421e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.316462  2359296  (512, 512, 3, 3)  ...  1525.471680        0.798005            0.363573   1.882730e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.316041  2359296  (512, 512, 3, 3)  ...  -839.860474        0.797446            0.363163   1.881412e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.315958  2359296  (512, 512, 3, 3)  ... -1041.307007        0.797033            0.362930   1.880438e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.316069  2359296  (512, 512, 3, 3)  ...  1765.372925        0.797603            0.363693   1.881781e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.316338  2359296  (512, 512, 3, 3)  ...  1207.909912        0.797300            0.363074   1.881067e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.305664     5120         (10, 512)  ...  -103.220398        0.802228            0.367777   4.107408e+03      True
27              fc    bias  1.000000       10             (10,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 99251360/313478154 (0.3166)
Testing time: 0.2169132709968835
remaining weights: 
0        529.999987
1         64.000000
2      11623.000122
3         64.000000
4      23389.000488
5        128.000000
6      46386.000000
7        128.000000
8      93296.003906
9        256.000000
10    187194.005859
11       256.000000
12    186097.007812
13       256.000000
14    373659.996094
15       512.000000
16    746627.976562
17       512.000000
18    745635.023438
19       512.000000
20    745438.007812
21       512.000000
22    745699.992188
23       512.000000
24    746335.968750
25       512.000000
26      1565.000000
27        10.000000
dtype: float64
flop each layer: [542719.986328125, 65536.0, 11901952.125, 65536.0, 5987584.125, 32768.0, 11874816.0, 32768.0, 5970944.25, 16384.0, 11980416.375, 16384.0, 11910208.5, 16384.0, 5978559.9375, 8192.0, 11946047.625, 8192.0, 11930160.375, 8192.0, 2981752.03125, 2048.0, 2982799.96875, 2048.0, 2985343.875, 2048.0, 1565.0, 10.0]
compression ratio: 0.5 ::: pruner: snipTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.461745          12.38          49.53
Final      200    0.062015   0.617267          88.68          98.92
Prune results:
             module   param  sparsity     size             shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.960069     1728     (64, 3, 3, 3)  ...   0.004098    2.371312e-06        8.008100e-12       0.004098      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.814914    36864    (64, 64, 3, 3)  ...   0.016441    4.459795e-07        5.062053e-13       0.016441      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.697632    73728   (128, 64, 3, 3)  ...   0.024368    3.305139e-07        3.882775e-13       0.024368      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.563890   147456  (128, 128, 3, 3)  ...   0.028571    1.937573e-07        1.677408e-13       0.028571      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.504662   294912  (256, 128, 3, 3)  ...   0.044500    1.508916e-07        1.019478e-13       0.044500      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.398092   589824  (256, 256, 3, 3)  ...   0.053705    9.105217e-08        4.141753e-14       0.053705      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.429882   589824  (256, 256, 3, 3)  ...   0.057504    9.749425e-08        4.256910e-14       0.057504      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.405670  1179648  (512, 256, 3, 3)  ...   0.098944    8.387602e-08        3.028708e-14       0.098944      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.303903  2359296  (512, 512, 3, 3)  ...   0.128534    5.447979e-08        1.533550e-14       0.128534      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.316752  2359296  (512, 512, 3, 3)  ...   0.132149    5.601207e-08        1.518338e-14       0.132149      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.319157  2359296  (512, 512, 3, 3)  ...   0.147987    6.272509e-08        2.044720e-14       0.147987      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.242681  2359296  (512, 512, 3, 3)  ...   0.123301    5.226167e-08        2.233858e-14       0.123301      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.244577  2359296  (512, 512, 3, 3)  ...   0.127362    5.398298e-08        2.504734e-14       0.127362      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.893750     5120         (10, 512)  ...   0.012537    2.448660e-06        1.678102e-11       0.012537      True
27              fc    bias  1.000000       10             (10,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 146671508/313478154 (0.4679)
Testing time: 0.2347494830028154
remaining weights: 
0       1658.999954
1         64.000000
2      30041.000244
3         64.000000
4      51435.000000
5        128.000000
6      83148.996094
7        128.000000
8     148830.996094
9        256.000000
10    234804.005859
11       256.000000
12    253554.996094
13       256.000000
14    478548.000000
15       512.000000
16    716995.968750
17       512.000000
18    747311.976562
19       512.000000
20    752985.000000
21       512.000000
22    572557.007812
23       512.000000
24    577028.988281
25       512.000000
26      4576.000061
27        10.000000
dtype: float64
flop each layer: [1698815.953125, 65536.0, 30761984.25, 65536.0, 13167360.0, 32768.0, 21286143.0, 32768.0, 9525183.75, 16384.0, 15027456.375, 16384.0, 16227519.75, 16384.0, 7656768.0, 8192.0, 11471935.5, 8192.0, 11956991.625, 8192.0, 3011940.0, 2048.0, 2290228.03125, 2048.0, 2308115.953125, 2048.0, 4576.000061035156, 10.0]
compression ratio: 0.5 ::: pruner: graspTrain results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN  2.417717e+00          11.73          50.17
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17
Post-Prune 0           NaN  5.379311e+07          10.00          50.00
Final      200    0.293041  1.040375e+00          72.92          97.82
Prune results:
             module   param  sparsity     size             shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.490162     1728     (64, 3, 3, 3)  ...   1.354097    4.320182e-02        4.537792e-03      74.652740      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.480903    36864    (64, 64, 3, 3)  ...  -0.472681    8.198874e-04        3.200053e-06      30.224329      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.452894    73728   (128, 64, 3, 3)  ...  -0.172852    5.155538e-04        1.557355e-06      38.010750      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.408285   147456  (128, 128, 3, 3)  ...  -0.310147    2.180302e-04        4.106673e-07      32.149868      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.389767   294912  (256, 128, 3, 3)  ...   0.064148    1.479369e-04        2.592434e-07      43.628365      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.354606   589824  (256, 256, 3, 3)  ...   0.162682    5.376640e-05        6.502705e-08      31.712711      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.379006   589824  (256, 256, 3, 3)  ...   0.162132    3.668722e-05        5.854170e-08      21.639000      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.372046  1179648  (512, 256, 3, 3)  ...   0.080144    1.399968e-05        1.085141e-08      16.514694      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.332920  2359296  (512, 512, 3, 3)  ...  -0.028948    3.009568e-06        1.246350e-09       7.100463      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.347864  2359296  (512, 512, 3, 3)  ...   0.026926    1.355132e-06        1.463858e-09       3.197158      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.308726  2359296  (512, 512, 3, 3)  ...  -0.093360    7.839813e-07        4.828354e-09       1.849644      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.256359  2359296  (512, 512, 3, 3)  ...  -0.094952    8.115064e-07        1.668920e-08       1.914584      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.259641  2359296  (512, 512, 3, 3)  ...   0.517122    6.257892e-07        3.254001e-08       1.476422      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.553125     5120         (10, 512)  ...  -0.194311    7.410139e-04        2.455229e-04       3.793991      True
27              fc    bias  1.000000       10             (10,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 4657711/14719818 (0.3164)
FLOP Sparsity: 118813942/313478154 (0.3790)
Testing time: 0.24039408399403328
remaining weights: 
0        847.000013
1         64.000000
2      17728.000488
3         64.000000
4      33390.999756
5        128.000000
6      60204.001465
7        128.000000
8     114946.998047
9        256.000000
10    209155.007812
11       256.000000
12    223546.992188
13       256.000000
14    438882.996094
15       512.000000
16    785457.000000
17       512.000000
18    820714.992188
19       512.000000
20    728374.992188
21       512.000000
22    604826.015625
23       512.000000
24    612570.023438
25       512.000000
26      2832.000122
27        10.000000
dtype: float64
flop each layer: [867328.013671875, 65536.0, 18153472.5, 65536.0, 8548095.9375, 32768.0, 15412224.375, 32768.0, 7356607.875, 16384.0, 13385920.5, 16384.0, 14307007.5, 16384.0, 7022127.9375, 8192.0, 12567312.0, 8192.0, 13131439.875, 8192.0, 2913499.96875, 2048.0, 2419304.0625, 2048.0, 2450280.09375, 2048.0, 2832.0001220703125, 10.0]
compression ratio: 0.5 ::: pruner: synflowTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.329107           9.99          51.19
Final      200    0.046446   0.821722          88.82          98.53
Prune results:
             module   param  sparsity     size             shape  ...     score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.997685     1728     (64, 3, 3, 3)  ...  2.945027e+22    1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.983805    36864    (64, 64, 3, 3)  ...  2.945027e+22    7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.969618    73728   (128, 64, 3, 3)  ...  2.945027e+22    3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.940477   147456  (128, 128, 3, 3)  ...  2.945027e+22    1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.881911   294912  (256, 128, 3, 3)  ...  2.945027e+22    9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.766754   589824  (256, 256, 3, 3)  ...  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.766447   589824  (256, 256, 3, 3)  ...  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.546590  1179648  (512, 256, 3, 3)  ...  2.945027e+22    2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.232446  2359296  (512, 512, 3, 3)  ...  2.945026e+22    1.248265e+16        9.402647e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.232694  2359296  (512, 512, 3, 3)  ...  2.945028e+22    1.248265e+16        9.415211e+31   2.945028e+22      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.211462  2359296  (512, 512, 3, 3)  ...  2.945027e+22    1.248265e+16                 inf   2.945027e+22      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.211558  2359296  (512, 512, 3, 3)  ...  2.945027e+22    1.248265e+16                 inf   2.945027e+22      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.210058  2359296  (512, 512, 3, 3)  ...  2.945028e+22    1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.997656     5120         (10, 512)  ...  2.945027e+22    5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10             (10,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 201358535/313478154 (0.6423)
Testing time: 0.23072773900639731
remaining weights: 
0       1724.000015
1         64.000000
2      36267.000732
3         64.000000
4      71488.001953
5        128.000000
6     138678.996094
7        128.000000
8     260086.007812
9        256.000000
10    452250.000000
11       256.000000
12    452069.015625
13       256.000000
14    644783.976562
15       512.000000
16    548408.988281
17       512.000000
18    548995.007812
19       512.000000
20    498901.992188
21       512.000000
22    499128.011719
23       512.000000
24    495587.988281
25       512.000000
26      5107.999878
27        10.000000
dtype: float64
flop each layer: [1765376.015625, 65536.0, 37137408.75, 65536.0, 18300928.5, 32768.0, 35501823.0, 32768.0, 16645504.5, 16384.0, 28944000.0, 16384.0, 28932417.0, 16384.0, 10316543.625, 8192.0, 8774543.8125, 8192.0, 8783920.125, 8192.0, 1995607.96875, 2048.0, 1996512.046875, 2048.0, 1982351.953125, 2048.0, 5107.9998779296875, 10.0]
