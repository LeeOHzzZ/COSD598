start!
compression ratio: 0.5 ::: pruner: magTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  200    0.043463   0.683562          88.79          98.36
Post-Prune 0           NaN   2.952472          69.71          97.46
Final      200    0.025590   0.800507          88.95          98.80
Prune results:
             module   param  sparsity     size             shape  ...      score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.835648     1728     (64, 3, 3, 3)  ...     404.176819        0.233899            0.030768     404.176819      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
2    layers.1.conv  weight  0.703369    36864    (64, 64, 3, 3)  ...    5255.583008        0.142567            0.014141    5255.583008      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
4    layers.3.conv  weight  0.718207    73728   (128, 64, 3, 3)  ...   11078.863281        0.150267            0.015122   11078.863281      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
6    layers.4.conv  weight  0.683085   147456  (128, 128, 3, 3)  ...   20347.138672        0.137988            0.013078   20347.138672      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
8    layers.6.conv  weight  0.678931   294912  (256, 128, 3, 3)  ...   40964.257812        0.138903            0.013515   40964.257812      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
10   layers.7.conv  weight  0.692861   589824  (256, 256, 3, 3)  ...   84195.765625        0.142747            0.013482   84195.765625      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
12   layers.8.conv  weight  0.676249   589824  (256, 256, 3, 3)  ...   81158.937500        0.137599            0.012790   81158.937500      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
14  layers.10.conv  weight  0.708377  1179648  (512, 256, 3, 3)  ...  164254.421875        0.139240            0.011652  164254.421875      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
16  layers.11.conv  weight  0.582944  2359296  (512, 512, 3, 3)  ...  241314.296875        0.102282            0.007571  241314.296875      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
18  layers.12.conv  weight  0.261677  2359296  (512, 512, 3, 3)  ...  117204.117188        0.049678            0.002977  117204.117188      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
20  layers.14.conv  weight  0.096645  2359296  (512, 512, 3, 3)  ...   67469.617188        0.028597            0.001055   67469.617188      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
22  layers.15.conv  weight  0.068717  2359296  (512, 512, 3, 3)  ...   59540.531250        0.025237            0.000718   59540.531250      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
24  layers.16.conv  weight  0.103406  2359296  (512, 512, 3, 3)  ...   69045.007812        0.029265            0.001006   69045.007812      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
26              fc  weight  0.433398     5120         (10, 512)  ...     422.240143        0.082469            0.008268     422.240143      True
27              fc    bias  1.000000       10             (10,)  ...       0.000000        0.000000            0.000000       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 179936509/313478154 (0.5740)
Testing time: 0.23282127199490787
remaining weights: 
0     1.444000e+03
1     6.400000e+01
2     2.592900e+04
3     6.400000e+01
4     5.295200e+04
5     1.280000e+02
6     1.007250e+05
7     1.280000e+02
8     2.002250e+05
9     2.560000e+02
10    4.086660e+05
11    2.560000e+02
12    3.988680e+05
13    2.560000e+02
14    8.356350e+05
15    5.120000e+02
16    1.375338e+06
17    5.120000e+02
18    6.173740e+05
19    5.120000e+02
20    2.280130e+05
21    5.120000e+02
22    1.621230e+05
23    5.120000e+02
24    2.439660e+05
25    5.120000e+02
26    2.219000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1478655.94921875, 65536.0, 26551296.0, 65536.0, 13555712.25, 32768.0, 25785600.75, 32768.0, 12814400.25, 16384.0, 26154623.25, 16384.0, 25527552.75, 16384.0, 13370160.375, 8192.0, 22005407.25, 8192.0, 9877983.75, 8192.0, 912051.984375, 2048.0, 648491.9765625, 2048.0, 975864.0234375, 2048.0, 2218.9999389648438, 10.0]
