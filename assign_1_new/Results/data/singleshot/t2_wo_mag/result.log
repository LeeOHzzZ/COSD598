start!
compression ratio: 0.05 ::: pruner: randTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.329417          10.01          51.04
Final      200    0.059478   0.679778          88.50          98.36
Prune results:
             module   param  sparsity     size             shape  ...    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.881944     1728     (64, 3, 3, 3)  ...   -46.742149        0.805873            0.356814   1.392548e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.892822    36864    (64, 64, 3, 3)  ...    49.684040        0.798002            0.364637   2.941756e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.893243    73728   (128, 64, 3, 3)  ...   293.166901        0.796496            0.363902   5.872405e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.891269   147456  (128, 128, 3, 3)  ...  -513.308838        0.797132            0.361494   1.175419e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.890628   294912  (256, 128, 3, 3)  ...    78.829033        0.800595            0.365375   2.361051e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.891902   589824  (256, 256, 3, 3)  ...  1204.694092        0.797834            0.363172   4.705814e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.890613   589824  (256, 256, 3, 3)  ... -1353.743652        0.798674            0.364395   4.710770e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.890803  1179648  (512, 256, 3, 3)  ...  -192.390137        0.799088            0.363946   9.426421e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.891143  2359296  (512, 512, 3, 3)  ...  1525.471680        0.798005            0.363573   1.882730e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.891075  2359296  (512, 512, 3, 3)  ...  -839.860474        0.797446            0.363163   1.881412e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.891395  2359296  (512, 512, 3, 3)  ... -1041.307007        0.797033            0.362930   1.880438e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.891385  2359296  (512, 512, 3, 3)  ...  1765.372925        0.797603            0.363693   1.881781e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.891476  2359296  (512, 512, 3, 3)  ...  1207.909912        0.797300            0.363074   1.881067e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.890430     5120         (10, 512)  ...  -103.220398        0.802228            0.367777   4.107408e+03      True
27              fc    bias  1.000000       10             (10,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 13119513/14719818 (0.8913)
FLOP Sparsity: 279473230/313478154 (0.8915)
Testing time: 0.2060499979997985
remaining weights: 
0     1.524000e+03
1     6.400000e+01
2     3.291300e+04
3     6.400000e+01
4     6.585700e+04
5     1.280000e+02
6     1.314230e+05
7     1.280000e+02
8     2.626570e+05
9     2.560000e+02
10    5.260650e+05
11    2.560000e+02
12    5.253050e+05
13    2.560000e+02
14    1.050834e+06
15    5.120000e+02
16    2.102471e+06
17    5.120000e+02
18    2.102310e+06
19    5.120000e+02
20    2.103064e+06
21    5.120000e+02
22    2.103042e+06
23    5.120000e+02
24    2.103255e+06
25    5.120000e+02
26    4.559000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1560575.953125, 65536.0, 33702912.0, 65536.0, 16859391.75, 32768.0, 33644288.25, 32768.0, 16810048.125, 16384.0, 33668160.75, 16384.0, 33619520.25, 16384.0, 16813344.375, 8192.0, 33639536.25, 8192.0, 33636960.0, 8192.0, 8412256.125, 2048.0, 8412167.8125, 2048.0, 8413020.0, 2048.0, 4558.999938964844, 10.0]
compression ratio: 0.05 ::: pruner: snipTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.417403          11.94          50.45
Final      200    0.161725   0.945564          77.32          96.50
Prune results:
             module   param  sparsity     size             shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  1.000000     1728     (64, 3, 3, 3)  ...   0.004098    2.371312e-06        8.008100e-12       0.004098      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  1.000000    36864    (64, 64, 3, 3)  ...   0.016441    4.459795e-07        5.062053e-13       0.016441      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.999959    73728   (128, 64, 3, 3)  ...   0.024368    3.305139e-07        3.882775e-13       0.024368      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.997342   147456  (128, 128, 3, 3)  ...   0.028571    1.937573e-07        1.677408e-13       0.028571      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.991489   294912  (256, 128, 3, 3)  ...   0.044500    1.508916e-07        1.019478e-13       0.044500      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.981010   589824  (256, 256, 3, 3)  ...   0.053705    9.105217e-08        4.141753e-14       0.053705      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.977183   589824  (256, 256, 3, 3)  ...   0.057504    9.749425e-08        4.256910e-14       0.057504      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.961767  1179648  (512, 256, 3, 3)  ...   0.098944    8.387602e-08        3.028708e-14       0.098944      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.934156  2359296  (512, 512, 3, 3)  ...   0.128534    5.447979e-08        1.533550e-14       0.128534      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.939188  2359296  (512, 512, 3, 3)  ...   0.132149    5.601207e-08        1.518338e-14       0.132149      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.884174  2359296  (512, 512, 3, 3)  ...   0.147987    6.272509e-08        2.044720e-14       0.147987      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.791248  2359296  (512, 512, 3, 3)  ...   0.123301    5.226167e-08        2.233858e-14       0.123301      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.803761  2359296  (512, 512, 3, 3)  ...   0.127362    5.398298e-08        2.504734e-14       0.127362      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.988281     5120         (10, 512)  ...   0.012537    2.448660e-06        1.678102e-11       0.012537      True
27              fc    bias  1.000000       10             (10,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 13119513/14719818 (0.8913)
FLOP Sparsity: 301220395/313478154 (0.9609)
Testing time: 0.22101779299555346
remaining weights: 
0     1.728000e+03
1     6.400000e+01
2     3.686400e+04
3     6.400000e+01
4     7.372500e+04
5     1.280000e+02
6     1.470640e+05
7     1.280000e+02
8     2.924020e+05
9     2.560000e+02
10    5.786230e+05
11    2.560000e+02
12    5.763660e+05
13    2.560000e+02
14    1.134547e+06
15    5.120000e+02
16    2.203950e+06
17    5.120000e+02
18    2.215823e+06
19    5.120000e+02
20    2.086029e+06
21    5.120000e+02
22    1.866788e+06
23    5.120000e+02
24    1.896310e+06
25    5.120000e+02
26    5.060000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1769472.0, 65536.0, 37748736.0, 65536.0, 18873599.625, 32768.0, 37648383.75, 32768.0, 18713728.125, 16384.0, 37031872.5, 16384.0, 36887424.75, 16384.0, 18152752.5, 8192.0, 35263199.25, 8192.0, 35453169.0, 8192.0, 8344116.0, 2048.0, 7467152.0625, 2048.0, 7585239.9375, 2048.0, 5060.0, 10.0]
compression ratio: 0.05 ::: pruner: graspTrain results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN  2.417717e+00          11.73          50.17
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17
Post-Prune 0           NaN  1.460584e+06          10.00          50.00
Final      200    0.123749  1.055288e+00          77.65          97.72
Prune results:
             module   param  sparsity     size             shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.506944     1728     (64, 3, 3, 3)  ...   0.518989    5.907374e-03        5.481607e-05      10.207941      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.502604    36864    (64, 64, 3, 3)  ...   0.069843    1.416330e-04        8.157956e-08       5.221159      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.508898    73728   (128, 64, 3, 3)  ...   0.064379    9.039466e-05        3.801878e-08       6.664618      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.532322   147456  (128, 128, 3, 3)  ...   0.014595    3.959083e-05        8.729521e-09       5.837905      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.539256   294912  (256, 128, 3, 3)  ...   0.029732    2.860111e-05        5.329184e-09       8.434810      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.591771   589824  (256, 256, 3, 3)  ...   0.034749    1.125159e-05        1.115928e-09       6.636457      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.649680   589824  (256, 256, 3, 3)  ...   0.025669    7.882853e-06        7.450295e-10       4.649496      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.751501  1179648  (512, 256, 3, 3)  ...   0.042152    2.843607e-06        1.780930e-10       3.354456      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.935817  2359296  (512, 512, 3, 3)  ...   0.000836    3.730548e-07        5.202561e-11       0.880147      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.953089  2359296  (512, 512, 3, 3)  ...   0.046494    2.022297e-07        1.903963e-10       0.477120      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.955258  2359296  (512, 512, 3, 3)  ...   0.020782    1.832842e-07        5.131428e-10       0.432422      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.954187  2359296  (512, 512, 3, 3)  ...  -0.114881    2.625644e-07        3.793312e-09       0.619467      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.948334  2359296  (512, 512, 3, 3)  ...   0.017399    1.862529e-07        2.366087e-09       0.439426      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.634375     5120         (10, 512)  ...   0.229261    1.698041e-04        2.806581e-06       0.869397      True
27              fc    bias  1.000000       10             (10,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 13119513/14719818 (0.8913)
FLOP Sparsity: 219347672/313478154 (0.6997)
Testing time: 0.20825688699551392
remaining weights: 
0     8.760000e+02
1     6.400000e+01
2     1.852800e+04
3     6.400000e+01
4     3.752000e+04
5     1.280000e+02
6     7.849400e+04
7     1.280000e+02
8     1.590330e+05
9     2.560000e+02
10    3.490410e+05
11    2.560000e+02
12    3.831970e+05
13    2.560000e+02
14    8.865070e+05
15    5.120000e+02
16    2.207869e+06
17    5.120000e+02
18    2.248619e+06
19    5.120000e+02
20    2.253737e+06
21    5.120000e+02
22    2.251210e+06
23    5.120000e+02
24    2.237400e+06
25    5.120000e+02
26    3.248000e+03
27    1.000000e+01
dtype: float64
flop each layer: [897023.953125, 65536.0, 18972672.75, 65536.0, 9605119.5, 32768.0, 20094464.25, 32768.0, 10178112.375, 16384.0, 22338623.25, 16384.0, 24524608.5, 16384.0, 14184112.5, 8192.0, 35325904.5, 8192.0, 35977905.0, 8192.0, 9014947.875, 2048.0, 9004839.75, 2048.0, 8949600.0, 2048.0, 3247.9998779296875, 10.0]
compression ratio: 0.05 ::: pruner: synflowTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.414332          11.66          50.38
Final      200    0.057811   0.793520          88.86          98.55
Prune results:
             module   param  sparsity     size             shape  ...     score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  1.000000     1728     (64, 3, 3, 3)  ...  2.945027e+22    1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.998264    36864    (64, 64, 3, 3)  ...  2.945027e+22    7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.996460    73728   (128, 64, 3, 3)  ...  2.945027e+22    3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.993198   147456  (128, 128, 3, 3)  ...  2.945027e+22    1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.986152   294912  (256, 128, 3, 3)  ...  2.945027e+22    9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.972612   589824  (256, 256, 3, 3)  ...  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.972772   589824  (256, 256, 3, 3)  ...  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.944177  1179648  (512, 256, 3, 3)  ...  2.945027e+22    2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.888635  2359296  (512, 512, 3, 3)  ...  2.945026e+22    1.248265e+16        9.402647e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.888531  2359296  (512, 512, 3, 3)  ...  2.945028e+22    1.248265e+16        9.415211e+31   2.945028e+22      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.865655  2359296  (512, 512, 3, 3)  ...  2.945027e+22    1.248265e+16                 inf   2.945027e+22      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.866078  2359296  (512, 512, 3, 3)  ...  2.945027e+22    1.248265e+16                 inf   2.945027e+22      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.856663  2359296  (512, 512, 3, 3)  ...  2.945028e+22    1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.999609     5120         (10, 512)  ...  2.945027e+22    5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10             (10,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 13119512/14719818 (0.8913)
FLOP Sparsity: 297416253/313478154 (0.9488)
Testing time: 0.21499803900223924
remaining weights: 
0     1.728000e+03
1     6.400000e+01
2     3.680000e+04
3     6.400000e+01
4     7.346700e+04
5     1.280000e+02
6     1.464530e+05
7     1.280000e+02
8     2.908280e+05
9     2.560000e+02
10    5.736700e+05
11    2.560000e+02
12    5.737640e+05
13    2.560000e+02
14    1.113797e+06
15    5.120000e+02
16    2.096552e+06
17    5.120000e+02
18    2.096308e+06
19    5.120000e+02
20    2.042337e+06
21    5.120000e+02
22    2.043335e+06
23    5.120000e+02
24    2.021122e+06
25    5.120000e+02
26    5.118000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1769472.0, 65536.0, 37683200.25, 65536.0, 18807552.0, 32768.0, 37491968.25, 32768.0, 18612992.25, 16384.0, 36714879.0, 16384.0, 36720895.5, 16384.0, 17820751.5, 8192.0, 33544831.5, 8192.0, 33540927.75, 8192.0, 8169347.8125, 2048.0, 8173339.875, 2048.0, 8084487.9375, 2048.0, 5117.9998779296875, 10.0]
compression ratio: 0.1 ::: pruner: randTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.310562          10.52          49.11
Final      200    0.046781   0.716100          88.72          98.48
Prune results:
             module   param  sparsity     size             shape  ...    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.789352     1728     (64, 3, 3, 3)  ...    -3.328420        0.815604            0.375404   1.409364e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.793891    36864    (64, 64, 3, 3)  ...   -41.070450        0.798311            0.367419   2.942894e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.793647    73728   (128, 64, 3, 3)  ...    22.698151        0.797265            0.361761   5.878076e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.796644   147456  (128, 128, 3, 3)  ...   365.462189        0.794750            0.363488   1.171907e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.792769   294912  (256, 128, 3, 3)  ...  -892.667786        0.799072            0.364270   2.356558e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.793304   589824  (256, 256, 3, 3)  ... -1864.469604        0.797693            0.364041   4.704983e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.793086   589824  (256, 256, 3, 3)  ...  -112.454193        0.799354            0.363792   4.714781e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.794735  1179648  (512, 256, 3, 3)  ...  -315.555328        0.797225            0.363389   9.404443e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.794568  2359296  (512, 512, 3, 3)  ...  -905.205139        0.797340            0.363327   1.881161e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.794430  2359296  (512, 512, 3, 3)  ... -1193.849243        0.797419            0.363391   1.881348e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.794182  2359296  (512, 512, 3, 3)  ...   278.497772        0.798120            0.363885   1.883002e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.794525  2359296  (512, 512, 3, 3)  ...  -717.875427        0.797913            0.364246   1.882513e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.794395  2359296  (512, 512, 3, 3)  ...  1459.083374        0.797887            0.363512   1.882453e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.787891     5120         (10, 512)  ...   -84.864716        0.798927            0.362136   4.090504e+03      True
27              fc    bias  1.000000       10             (10,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 11693237/14719818 (0.7944)
FLOP Sparsity: 249017342/313478154 (0.7944)
Testing time: 0.2083919610013254
remaining weights: 
0     1.364000e+03
1     6.400000e+01
2     2.926600e+04
3     6.400000e+01
4     5.851400e+04
5     1.280000e+02
6     1.174700e+05
7     1.280000e+02
8     2.337970e+05
9     2.560000e+02
10    4.679100e+05
11    2.560000e+02
12    4.677810e+05
13    2.560000e+02
14    9.375080e+05
15    5.120000e+02
16    1.874621e+06
17    5.120000e+02
18    1.874295e+06
19    5.120000e+02
20    1.873711e+06
21    5.120000e+02
22    1.874519e+06
23    5.120000e+02
24    1.874214e+06
25    5.120000e+02
26    4.034000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1396736.05078125, 65536.0, 29968384.5, 65536.0, 14979584.25, 32768.0, 30072321.0, 32768.0, 14963008.5, 16384.0, 29946240.0, 16384.0, 29937984.75, 16384.0, 15000127.875, 8192.0, 29993935.5, 8192.0, 29988720.0, 8192.0, 7494843.9375, 2048.0, 7498076.0625, 2048.0, 7496856.0, 2048.0, 4033.9999389648438, 10.0]
compression ratio: 0.1 ::: pruner: snipTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.430576          11.47          49.47
Final      200    0.055095   0.626178          88.10          98.95
Prune results:
             module   param  sparsity     size             shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  1.000000     1728     (64, 3, 3, 3)  ...   0.003898    2.255773e-06        7.990470e-12       0.003898      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.999295    36864    (64, 64, 3, 3)  ...   0.015996    4.339102e-07        5.039090e-13       0.015996      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.997694    73728   (128, 64, 3, 3)  ...   0.023353    3.167424e-07        3.276922e-13       0.023353      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.991225   147456  (128, 128, 3, 3)  ...   0.027807    1.885813e-07        1.471547e-13       0.027807      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.981635   294912  (256, 128, 3, 3)  ...   0.042642    1.445915e-07        9.236275e-14       0.042642      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.961597   589824  (256, 256, 3, 3)  ...   0.053609    9.089010e-08        4.306487e-14       0.053609      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.955046   589824  (256, 256, 3, 3)  ...   0.058194    9.866352e-08        4.493353e-14       0.058194      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.934725  1179648  (512, 256, 3, 3)  ...   0.102035    8.649634e-08        3.203747e-14       0.102035      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.892130  2359296  (512, 512, 3, 3)  ...   0.130947    5.550256e-08        1.558845e-14       0.130947      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.887398  2359296  (512, 512, 3, 3)  ...   0.130829    5.545267e-08        1.463762e-14       0.130829      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.760796  2359296  (512, 512, 3, 3)  ...   0.146354    6.203295e-08        1.995140e-14       0.146354      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.616012  2359296  (512, 512, 3, 3)  ...   0.123699    5.243040e-08        2.241346e-14       0.123699      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.617262  2359296  (512, 512, 3, 3)  ...   0.127790    5.416435e-08        2.529172e-14       0.127790      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.988086     5120         (10, 512)  ...   0.012847    2.509150e-06        1.771344e-11       0.012847      True
27              fc    bias  1.000000       10             (10,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 11693238/14719818 (0.7944)
FLOP Sparsity: 290535731/313478154 (0.9268)
Testing time: 0.21251411500270478
remaining weights: 
0     1.728000e+03
1     6.400000e+01
2     3.683800e+04
3     6.400000e+01
4     7.355800e+04
5     1.280000e+02
6     1.461620e+05
7     1.280000e+02
8     2.894960e+05
9     2.560000e+02
10    5.671730e+05
11    2.560000e+02
12    5.633090e+05
13    2.560000e+02
14    1.102647e+06
15    5.120000e+02
16    2.104798e+06
17    5.120000e+02
18    2.093635e+06
19    5.120000e+02
20    1.794942e+06
21    5.120000e+02
22    1.453355e+06
23    5.120000e+02
24    1.456304e+06
25    5.120000e+02
26    5.059000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1769472.0, 65536.0, 37722111.75, 65536.0, 18830848.5, 32768.0, 37417473.0, 32768.0, 18527744.25, 16384.0, 36299072.25, 16384.0, 36051777.0, 16384.0, 17642352.375, 8192.0, 33676767.0, 8192.0, 33498159.75, 8192.0, 7179768.0, 2048.0, 5813420.0625, 2048.0, 5825216.25, 2048.0, 5058.999938964844, 10.0]
compression ratio: 0.1 ::: pruner: graspTrain results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN  2.417717e+00          11.73          50.17
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17
Post-Prune 0           NaN  1.256377e+08          10.00          50.00
Final      200    0.221922  1.160783e+00          73.01          97.46
Prune results:
             module   param  sparsity     size             shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.492477     1728     (64, 3, 3, 3)  ...  -1.125460    9.877865e-03        1.943442e-04      17.068951      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.505263    36864    (64, 64, 3, 3)  ...   0.032678    1.924871e-04        1.712617e-07       7.095844      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.506361    73728   (128, 64, 3, 3)  ...  -0.065130    1.292817e-04        9.202142e-08       9.531679      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.528225   147456  (128, 128, 3, 3)  ...   0.057328    5.100102e-05        1.706852e-08       7.520407      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.540955   294912  (256, 128, 3, 3)  ...   0.038156    3.173263e-05        9.530517e-09       9.358334      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.586933   589824  (256, 256, 3, 3)  ...  -0.025342    9.279179e-06        1.992106e-09       5.473083      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.647722   589824  (256, 256, 3, 3)  ...   0.044651    5.424839e-06        1.234015e-09       3.199701      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.703053  1179648  (512, 256, 3, 3)  ...  -0.012908    1.626328e-06        2.062242e-10       1.918494      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.821067  2359296  (512, 512, 3, 3)  ...  -0.005882    2.045397e-07        6.404324e-11       0.482570      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.825024  2359296  (512, 512, 3, 3)  ...  -0.001479    8.250755e-08        4.697980e-11       0.194660      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.832673  2359296  (512, 512, 3, 3)  ...   0.014412    5.939373e-08        8.766955e-11       0.140127      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.849207  2359296  (512, 512, 3, 3)  ...   0.041744    6.004437e-08        1.880610e-10       0.141662      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.840320  2359296  (512, 512, 3, 3)  ...   0.010013    4.313110e-08        1.299138e-10       0.101759      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.576953     5120         (10, 512)  ...  -0.002782    3.452536e-05        3.945467e-07       0.176770      True
27              fc    bias  1.000000       10             (10,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 11693238/14719818 (0.7944)
FLOP Sparsity: 205747801/313478154 (0.6563)
Testing time: 0.210758987988811
remaining weights: 
0     8.510000e+02
1     6.400000e+01
2     1.862600e+04
3     6.400000e+01
4     3.733300e+04
5     1.280000e+02
6     7.789000e+04
7     1.280000e+02
8     1.595340e+05
9     2.560000e+02
10    3.461870e+05
11    2.560000e+02
12    3.820420e+05
13    2.560000e+02
14    8.293550e+05
15    5.120000e+02
16    1.937139e+06
17    5.120000e+02
18    1.946476e+06
19    5.120000e+02
20    1.964523e+06
21    5.120000e+02
22    2.003531e+06
23    5.120000e+02
24    1.982563e+06
25    5.120000e+02
26    2.954000e+03
27    1.000000e+01
dtype: float64
flop each layer: [871423.998046875, 65536.0, 19073025.0, 65536.0, 9557248.5, 32768.0, 19939839.75, 32768.0, 10210176.0, 16384.0, 22155968.25, 16384.0, 24450687.0, 16384.0, 13269679.875, 8192.0, 30994224.75, 8192.0, 31143615.75, 8192.0, 7858091.8125, 2048.0, 8014124.25, 2048.0, 7930252.125, 2048.0, 2953.9999389648438, 10.0]
compression ratio: 0.1 ::: pruner: synflowTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.403529          11.45          49.92
Final      200    0.059073   0.535263          86.78          98.50
Prune results:
             module   param  sparsity     size             shape  ...     score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.999421     1728     (64, 3, 3, 3)  ...  2.945027e+22    1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.996582    36864    (64, 64, 3, 3)  ...  2.945027e+22    7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.993096    73728   (128, 64, 3, 3)  ...  2.945027e+22    3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.986925   147456  (128, 128, 3, 3)  ...  2.945027e+22    1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.974199   294912  (256, 128, 3, 3)  ...  2.945027e+22    9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.947367   589824  (256, 256, 3, 3)  ...  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.947562   589824  (256, 256, 3, 3)  ...  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.892713  1179648  (512, 256, 3, 3)  ...  2.945027e+22    2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.788353  2359296  (512, 512, 3, 3)  ...  2.945026e+22    1.248265e+16        9.402647e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.787956  2359296  (512, 512, 3, 3)  ...  2.945028e+22    1.248265e+16        9.415211e+31   2.945028e+22      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.746524  2359296  (512, 512, 3, 3)  ...  2.945027e+22    1.248265e+16                 inf   2.945027e+22      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.746590  2359296  (512, 512, 3, 3)  ...  2.945027e+22    1.248265e+16                 inf   2.945027e+22      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.731970  2359296  (512, 512, 3, 3)  ...  2.945028e+22    1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.999609     5120         (10, 512)  ...  2.945027e+22    5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10             (10,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 11693238/14719818 (0.7944)
FLOP Sparsity: 282939167/313478154 (0.9026)
Testing time: 0.21990904801350553
remaining weights: 
0     1.727000e+03
1     6.400000e+01
2     3.673800e+04
3     6.400000e+01
4     7.321900e+04
5     1.280000e+02
6     1.455280e+05
7     1.280000e+02
8     2.873030e+05
9     2.560000e+02
10    5.587800e+05
11    2.560000e+02
12    5.588950e+05
13    2.560000e+02
14    1.053087e+06
15    5.120000e+02
16    1.859957e+06
17    5.120000e+02
18    1.859022e+06
19    5.120000e+02
20    1.761270e+06
21    5.120000e+02
22    1.761426e+06
23    5.120000e+02
24    1.726934e+06
25    5.120000e+02
26    5.118000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1768448.00390625, 65536.0, 37619712.0, 65536.0, 18744063.75, 32768.0, 37255167.0, 32768.0, 18387391.5, 16384.0, 35761920.75, 16384.0, 35769280.5, 16384.0, 16849391.625, 8192.0, 29759312.25, 8192.0, 29744352.0, 8192.0, 7045080.1875, 2048.0, 7045704.0, 2048.0, 6907736.25, 2048.0, 5117.9998779296875, 10.0]
compression ratio: 0.2 ::: pruner: randTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.302677           9.88          49.18
Final      200    0.039883   0.978842          87.97          98.68
Prune results:
             module   param  sparsity     size             shape  ...    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.641204     1728     (64, 3, 3, 3)  ...    61.327629        0.794965            0.357390   1.373700e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.628581    36864    (64, 64, 3, 3)  ...   -86.159767        0.800778            0.367302   2.951989e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.628825    73728   (128, 64, 3, 3)  ...   -40.442982        0.795844            0.360363   5.867596e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.635159   147456  (128, 128, 3, 3)  ...   950.081726        0.798743            0.364650   1.177794e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.630998   294912  (256, 128, 3, 3)  ...  -100.452507        0.800204            0.366403   2.359898e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.629988   589824  (256, 256, 3, 3)  ... -1526.321411        0.799142            0.364628   4.713528e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.631068   589824  (256, 256, 3, 3)  ...    11.105591        0.798349            0.364008   4.708857e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.630990  1179648  (512, 256, 3, 3)  ...  -652.789734        0.797750            0.363370   9.410644e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.631371  2359296  (512, 512, 3, 3)  ...  -410.586914        0.797856            0.363783   1.882378e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.630685  2359296  (512, 512, 3, 3)  ... -1211.686279        0.798607            0.363021   1.884149e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.631265  2359296  (512, 512, 3, 3)  ...  -108.137291        0.798046            0.363955   1.882828e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.630630  2359296  (512, 512, 3, 3)  ... -3071.898193        0.797777            0.363521   1.882193e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.630859  2359296  (512, 512, 3, 3)  ...  -344.696045        0.797719            0.362802   1.882054e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.632812     5120         (10, 512)  ...   -22.838730        0.808849            0.376089   4.141307e+03      True
27              fc    bias  1.000000       10             (10,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 9289139/14719818 (0.6311)
FLOP Sparsity: 197913366/313478154 (0.6313)
Testing time: 0.21717314899433404
remaining weights: 
0     1.108000e+03
1     6.400000e+01
2     2.317200e+04
3     6.400000e+01
4     4.636200e+04
5     1.280000e+02
6     9.365800e+04
7     1.280000e+02
8     1.860890e+05
9     2.560000e+02
10    3.715820e+05
11    2.560000e+02
12    3.722190e+05
13    2.560000e+02
14    7.443460e+05
15    5.120000e+02
16    1.489592e+06
17    5.120000e+02
18    1.487973e+06
19    5.120000e+02
20    1.489340e+06
21    5.120000e+02
22    1.487843e+06
23    5.120000e+02
24    1.488382e+06
25    5.120000e+02
26    3.240000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1134591.99609375, 65536.0, 23728128.75, 65536.0, 11868672.375, 32768.0, 23976447.75, 32768.0, 11909695.5, 16384.0, 23781249.0, 16384.0, 23822016.75, 16384.0, 11909535.75, 8192.0, 23833471.5, 8192.0, 23807567.25, 8192.0, 5957359.875, 2048.0, 5951372.0625, 2048.0, 5953528.125, 2048.0, 3240.0, 10.0]
compression ratio: 0.2 ::: pruner: snipTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.421687           9.26          50.94
Final      200    0.311907   0.567843          84.85          98.78
Prune results:
             module   param  sparsity     size             shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.994213     1728     (64, 3, 3, 3)  ...   0.003690    2.135669e-06        7.092938e-12       0.003690      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.965224    36864    (64, 64, 3, 3)  ...   0.014240    3.862914e-07        3.912767e-13       0.014240      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.932848    73728   (128, 64, 3, 3)  ...   0.021418    2.904997e-07        2.651307e-13       0.021418      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.872674   147456  (128, 128, 3, 3)  ...   0.026668    1.808536e-07        1.316367e-13       0.026668      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.838230   294912  (256, 128, 3, 3)  ...   0.039263    1.331332e-07        7.618787e-14       0.039263      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.766437   589824  (256, 256, 3, 3)  ...   0.050483    8.558916e-08        3.477215e-14       0.050483      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.791862   589824  (256, 256, 3, 3)  ...   0.056382    9.559101e-08        4.060944e-14       0.056382      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.774792  1179648  (512, 256, 3, 3)  ...   0.103639    8.785588e-08        3.286323e-14       0.103639      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.676737  2359296  (512, 512, 3, 3)  ...   0.133091    5.641143e-08        1.661132e-14       0.133091      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.692954  2359296  (512, 512, 3, 3)  ...   0.133340    5.651706e-08        1.485652e-14       0.133340      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.615973  2359296  (512, 512, 3, 3)  ...   0.149742    6.346902e-08        2.097866e-14       0.149742      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.484752  2359296  (512, 512, 3, 3)  ...   0.125628    5.324816e-08        2.292988e-14       0.125628      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.481689  2359296  (512, 512, 3, 3)  ...   0.129689    5.496941e-08        2.554656e-14       0.129689      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.967188     5120         (10, 512)  ...   0.012726    2.485597e-06        1.739077e-11       0.012726      True
27              fc    bias  1.000000       10             (10,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 9289140/14719818 (0.6311)
FLOP Sparsity: 244932078/313478154 (0.7813)
Testing time: 0.21331604299484752
remaining weights: 
0     1.718000e+03
1     6.400000e+01
2     3.558200e+04
3     6.400000e+01
4     6.877700e+04
5     1.280000e+02
6     1.286810e+05
7     1.280000e+02
8     2.472040e+05
9     2.560000e+02
10    4.520630e+05
11    2.560000e+02
12    4.670590e+05
13    2.560000e+02
14    9.139820e+05
15    5.120000e+02
16    1.596622e+06
17    5.120000e+02
18    1.634883e+06
19    5.120000e+02
20    1.453262e+06
21    5.120000e+02
22    1.143674e+06
23    5.120000e+02
24    1.136447e+06
25    5.120000e+02
26    4.952000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1759232.0390625, 65536.0, 36435969.0, 65536.0, 17606911.5, 32768.0, 32942335.5, 32768.0, 15821056.125, 16384.0, 28932032.25, 16384.0, 29891776.5, 16384.0, 14623711.875, 8192.0, 25545951.0, 8192.0, 26158128.75, 8192.0, 5813048.25, 2048.0, 4574696.0625, 2048.0, 4545788.0625, 2048.0, 4952.0001220703125, 10.0]
compression ratio: 0.2 ::: pruner: graspTrain results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN  2.417717e+00          11.73          50.17
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17
Post-Prune 0           NaN  7.049756e+07          10.00          50.00
Final      200    0.163434  9.926005e-01          77.17          97.93
Prune results:
             module   param  sparsity     size             shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.509838     1728     (64, 3, 3, 3)  ...   2.951932        0.057097        6.352426e-03      98.664047      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.502089    36864    (64, 64, 3, 3)  ...   0.111038        0.001233        5.584574e-06      45.453117      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.511529    73728   (128, 64, 3, 3)  ...   0.038273        0.000863        3.064408e-06      63.597183      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.529758   147456  (128, 128, 3, 3)  ...  -0.648744        0.000355        8.123010e-07      52.295921      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.539666   294912  (256, 128, 3, 3)  ...   0.033410        0.000269        3.690398e-07      79.430840      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.561010   589824  (256, 256, 3, 3)  ...   0.046274        0.000105        9.271484e-08      61.955803      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.551461   589824  (256, 256, 3, 3)  ...   0.162850        0.000071        6.803980e-08      41.818279      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.564311  1179648  (512, 256, 3, 3)  ...  -0.208473        0.000023        1.753721e-08      27.622557      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.594898  2359296  (512, 512, 3, 3)  ...  -0.074857        0.000003        4.742948e-09       7.922766      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.590898  2359296  (512, 512, 3, 3)  ...  -0.015659        0.000002        1.056274e-08       4.367321      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.645261  2359296  (512, 512, 3, 3)  ...  -0.077397        0.000001        2.163724e-08       3.047753      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.710593  2359296  (512, 512, 3, 3)  ...  -0.012817        0.000001        8.355434e-08       3.241338      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.707521  2359296  (512, 512, 3, 3)  ...  -0.416338        0.000001        1.040737e-07       2.702241      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
26              fc  weight  0.571289     5120         (10, 512)  ...  -0.889492        0.001341        2.298124e-04       6.865910      True
27              fc    bias  1.000000       10             (10,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 9289139/14719818 (0.6311)
FLOP Sparsity: 176853253/313478154 (0.5642)
Testing time: 0.23103396099759266
remaining weights: 
0     8.810000e+02
1     6.400000e+01
2     1.850900e+04
3     6.400000e+01
4     3.771400e+04
5     1.280000e+02
6     7.811600e+04
7     1.280000e+02
8     1.591540e+05
9     2.560000e+02
10    3.308970e+05
11    2.560000e+02
12    3.252650e+05
13    2.560000e+02
14    6.656880e+05
15    5.120000e+02
16    1.403541e+06
17    5.120000e+02
18    1.394104e+06
19    5.120000e+02
20    1.522361e+06
21    5.120000e+02
22    1.676499e+06
23    5.120000e+02
24    1.669252e+06
25    5.120000e+02
26    2.925000e+03
27    1.000000e+01
dtype: float64
flop each layer: [902144.0390625, 65536.0, 18953217.0, 65536.0, 9654783.75, 32768.0, 19997696.25, 32768.0, 10185855.75, 16384.0, 21177407.25, 16384.0, 20816959.5, 16384.0, 10651008.375, 8192.0, 22456656.0, 8192.0, 22305663.0, 8192.0, 6089443.875, 2048.0, 6705996.1875, 2048.0, 6677007.75, 2048.0, 2925.0, 10.0]
compression ratio: 0.2 ::: pruner: synflowTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.396133          10.04          51.07
Final      200    0.048394   0.690013          88.24          98.80
Prune results:
             module   param  sparsity     size             shape  ...     score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.998843     1728     (64, 3, 3, 3)  ...  2.945027e+22    1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.993490    36864    (64, 64, 3, 3)  ...  2.945027e+22    7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.987115    73728   (128, 64, 3, 3)  ...  2.945027e+22    3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.974752   147456  (128, 128, 3, 3)  ...  2.945027e+22    1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.950578   294912  (256, 128, 3, 3)  ...  2.945027e+22    9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.900650   589824  (256, 256, 3, 3)  ...  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.900650   589824  (256, 256, 3, 3)  ...  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.799322  1179648  (512, 256, 3, 3)  ...  2.945027e+22    2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.611696  2359296  (512, 512, 3, 3)  ...  2.945026e+22    1.248265e+16        9.402647e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.611218  2359296  (512, 512, 3, 3)  ...  2.945028e+22    1.248265e+16        9.415211e+31   2.945028e+22      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.549993  2359296  (512, 512, 3, 3)  ...  2.945027e+22    1.248265e+16                 inf   2.945027e+22      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.550157  2359296  (512, 512, 3, 3)  ...  2.945027e+22    1.248265e+16                 inf   2.945027e+22      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.533390  2359296  (512, 512, 3, 3)  ...  2.945028e+22    1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.999219     5120         (10, 512)  ...  2.945027e+22    5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10             (10,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 9289138/14719818 (0.6311)
FLOP Sparsity: 257583382/313478154 (0.8217)
Testing time: 0.2236451950011542
remaining weights: 
0     1.726000e+03
1     6.400000e+01
2     3.662400e+04
3     6.400000e+01
4     7.277800e+04
5     1.280000e+02
6     1.437330e+05
7     1.280000e+02
8     2.803370e+05
9     2.560000e+02
10    5.312250e+05
11    2.560000e+02
12    5.312250e+05
13    2.560000e+02
14    9.429190e+05
15    5.120000e+02
16    1.443172e+06
17    5.120000e+02
18    1.442045e+06
19    5.120000e+02
20    1.297596e+06
21    5.120000e+02
22    1.297983e+06
23    5.120000e+02
24    1.258426e+06
25    5.120000e+02
26    5.116000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1767424.0078125, 65536.0, 37502975.25, 65536.0, 18631167.75, 32768.0, 36795647.25, 32768.0, 17941567.5, 16384.0, 33998400.0, 16384.0, 33998400.0, 16384.0, 15086704.5, 8192.0, 23090751.0, 8192.0, 23072719.5, 8192.0, 5190383.8125, 2048.0, 5191931.8125, 2048.0, 5033703.9375, 2048.0, 5116.000061035156, 10.0]
compression ratio: 0.5 ::: pruner: randTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.302587          10.00          50.89
Final      200    0.036758   0.828924          87.97          99.00
Prune results:
             module   param  sparsity     size             shape  ...    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.295718     1728     (64, 3, 3, 3)  ...   -55.828125        0.781616            0.377116   1.350632e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.316243    36864    (64, 64, 3, 3)  ...     7.592377        0.797182            0.361089   2.938732e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.316555    73728   (128, 64, 3, 3)  ...   258.599121        0.797825            0.362435   5.882200e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.315877   147456  (128, 128, 3, 3)  ...   -64.451935        0.798079            0.363249   1.176815e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.316552   294912  (256, 128, 3, 3)  ...   573.935913        0.795586            0.361879   2.346279e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.316055   589824  (256, 256, 3, 3)  ...   -35.837318        0.797746            0.363137   4.705296e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.315899   589824  (256, 256, 3, 3)  ...   355.280365        0.797960            0.363405   4.706558e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.316657  1179648  (512, 256, 3, 3)  ...   954.266357        0.799225            0.364781   9.428042e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.316356  2359296  (512, 512, 3, 3)  ...  1749.886108        0.798217            0.363900   1.883230e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.316095  2359296  (512, 512, 3, 3)  ... -1354.823242        0.798736            0.363697   1.884454e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.316305  2359296  (512, 512, 3, 3)  ...   902.733459        0.798635            0.364066   1.884217e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.316805  2359296  (512, 512, 3, 3)  ...  2136.738037        0.798660            0.363893   1.884275e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.315464  2359296  (512, 512, 3, 3)  ... -3668.028320        0.798003            0.363786   1.882725e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.321094     5120         (10, 512)  ...     4.917744        0.810438            0.362392   4.149442e+03      True
27              fc    bias  1.000000       10             (10,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 4657711/14719818 (0.3164)
FLOP Sparsity: 99270958/313478154 (0.3167)
Testing time: 0.21439294300216716
remaining weights: 
0        511.000008
1         64.000000
2      11658.000366
3         64.000000
4      23338.999512
5        128.000000
6      46578.001465
7        128.000000
8      93354.996094
9        256.000000
10    186417.000000
11       256.000000
12    186324.996094
13       256.000000
14    373544.015625
15       512.000000
16    746378.015625
17       512.000000
18    745762.007812
19       512.000000
20    746257.007812
21       512.000000
22    747436.992188
23       512.000000
24    744272.015625
25       512.000000
26      1643.999939
27        10.000000
dtype: float64
flop each layer: [523264.0078125, 65536.0, 11937792.375, 65536.0, 5974783.875, 32768.0, 11923968.375, 32768.0, 5974719.75, 16384.0, 11930688.0, 16384.0, 11924799.75, 16384.0, 5976704.25, 8192.0, 11942048.25, 8192.0, 11932192.125, 8192.0, 2985028.03125, 2048.0, 2989747.96875, 2048.0, 2977088.0625, 2048.0, 1643.9999389648438, 10.0]
compression ratio: 0.5 ::: pruner: snipTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.425261           9.29          50.30
Final      200    0.041667   0.641720          88.62          99.07
Prune results:
             module   param  sparsity     size             shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.960648     1728     (64, 3, 3, 3)  ...   0.003904    2.259309e-06        7.893469e-12       0.003904      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.794162    36864    (64, 64, 3, 3)  ...   0.015731    4.267337e-07        5.187077e-13       0.015731      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.692084    73728   (128, 64, 3, 3)  ...   0.022851    3.099347e-07        3.074969e-13       0.022851      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.557102   147456  (128, 128, 3, 3)  ...   0.027192    1.844101e-07        1.409751e-13       0.027192      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.491069   294912  (256, 128, 3, 3)  ...   0.041389    1.403443e-07        8.803299e-14       0.041389      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.387619   589824  (256, 256, 3, 3)  ...   0.050438    8.551374e-08        3.557064e-14       0.050438      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.423894   589824  (256, 256, 3, 3)  ...   0.056469    9.573907e-08        4.306391e-14       0.056469      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.413686  1179648  (512, 256, 3, 3)  ...   0.104655    8.871735e-08        3.398662e-14       0.104655      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.303082  2359296  (512, 512, 3, 3)  ...   0.129560    5.491488e-08        1.593610e-14       0.129560      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.316693  2359296  (512, 512, 3, 3)  ...   0.132445    5.613762e-08        1.506529e-14       0.132445      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.322163  2359296  (512, 512, 3, 3)  ...   0.149906    6.353825e-08        2.068848e-14       0.149906      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.242002  2359296  (512, 512, 3, 3)  ...   0.123918    5.252342e-08        2.271081e-14       0.123918      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.245855  2359296  (512, 512, 3, 3)  ...   0.128789    5.458782e-08        2.537726e-14       0.128789      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.895117     5120         (10, 512)  ...   0.012751    2.490483e-06        1.717699e-11       0.012751      True
27              fc    bias  1.000000       10             (10,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 144802381/313478154 (0.4619)
Testing time: 0.21049962799588684
remaining weights: 
0       1659.999950
1         64.000000
2      29276.000244
3         64.000000
4      51026.000977
5        128.000000
6      82148.000977
7        128.000000
8     144821.997070
9        256.000000
10    228627.000000
11       256.000000
12    250023.005859
13       256.000000
14    488004.011719
15       512.000000
16    715059.984375
17       512.000000
18    747172.968750
19       512.000000
20    760077.000000
21       512.000000
22    570953.988281
23       512.000000
24    580043.988281
25       512.000000
26      4582.999878
27        10.000000
dtype: float64
flop each layer: [1699839.94921875, 65536.0, 29978624.25, 65536.0, 13062656.25, 32768.0, 21029888.25, 32768.0, 9268607.8125, 16384.0, 14632128.0, 16384.0, 16001472.375, 16384.0, 7808064.1875, 8192.0, 11440959.75, 8192.0, 11954767.5, 8192.0, 3040308.0, 2048.0, 2283815.953125, 2048.0, 2320175.953125, 2048.0, 4582.9998779296875, 10.0]
compression ratio: 0.5 ::: pruner: graspTrain results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN  2.417717e+00          11.73          50.17
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17
Post-Prune 0           NaN  1.875452e+08          10.00          50.00
Final      200    0.237272  8.442642e-01          79.52          98.62
Prune results:
             module   param  sparsity     size             shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.489005     1728     (64, 3, 3, 3)  ...   0.768485    8.368349e-02        1.218657e-02     144.605072      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.487332    36864    (64, 64, 3, 3)  ...   1.822769    1.789120e-03        1.662878e-05      65.954109      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.460069    73728   (128, 64, 3, 3)  ...   0.281393    1.308053e-03        1.012644e-05      96.440155      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.410129   147456  (128, 128, 3, 3)  ...  -0.256279    5.060481e-04        1.959665e-06      74.619835      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.389926   294912  (256, 128, 3, 3)  ...   0.192848    3.435705e-04        9.961741e-07     101.323051      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.354158   589824  (256, 256, 3, 3)  ...  -0.285168    1.166226e-04        2.425178e-07      68.786819      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.377804   589824  (256, 256, 3, 3)  ...   0.212257    7.599989e-05        1.700045e-07      44.826557      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.367922  1179648  (512, 256, 3, 3)  ...  -0.045776    2.978019e-05        3.929177e-08      35.130142      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.331249  2359296  (512, 512, 3, 3)  ...  -0.130686    6.052019e-06        7.043255e-09      14.278504      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.345879  2359296  (512, 512, 3, 3)  ...  -0.055943    2.935800e-06        7.097194e-09       6.926421      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.307273  2359296  (512, 512, 3, 3)  ...   0.212176    1.447602e-06        9.273553e-09       3.415322      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.258292  2359296  (512, 512, 3, 3)  ...  -0.330903    1.206675e-06        3.415375e-08       2.846904      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.264839  2359296  (512, 512, 3, 3)  ...  -0.798484    9.974696e-07        8.232643e-08       2.353326      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.549805     5120         (10, 512)  ...  -0.586689    1.108654e-03        3.747671e-04       5.676308      True
27              fc    bias  1.000000       10             (10,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 119038104/313478154 (0.3797)
Testing time: 0.2125538050022442
remaining weights: 
0        845.000021
1         64.000000
2      17964.999756
3         64.000000
4      33920.000244
5        128.000000
6      60476.000977
7        128.000000
8     114994.001953
9        256.000000
10    208891.001953
11       256.000000
12    222837.996094
13       256.000000
14    434018.003906
15       512.000000
16    781515.000000
17       512.000000
18    816031.968750
19       512.000000
20    724946.976562
21       512.000000
22    609388.031250
23       512.000000
24    624833.015625
25       512.000000
26      2815.000000
27        10.000000
dtype: float64
flop each layer: [865280.021484375, 65536.0, 18396159.75, 65536.0, 8683520.0625, 32768.0, 15481856.25, 32768.0, 7359616.125, 16384.0, 13369024.125, 16384.0, 14261631.75, 16384.0, 6944288.0625, 8192.0, 12504240.0, 8192.0, 13056511.5, 8192.0, 2899787.90625, 2048.0, 2437552.125, 2048.0, 2499332.0625, 2048.0, 2815.0, 10.0]
compression ratio: 0.5 ::: pruner: synflowTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.329107           9.99          51.19
Final      200    0.041721   0.693759          88.34          98.68
Prune results:
             module   param  sparsity     size             shape  ...     score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.997685     1728     (64, 3, 3, 3)  ...  2.945027e+22    1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.983805    36864    (64, 64, 3, 3)  ...  2.945027e+22    7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.969618    73728   (128, 64, 3, 3)  ...  2.945027e+22    3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.940477   147456  (128, 128, 3, 3)  ...  2.945027e+22    1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.881911   294912  (256, 128, 3, 3)  ...  2.945027e+22    9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.766754   589824  (256, 256, 3, 3)  ...  2.945027e+22    4.993060e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.766447   589824  (256, 256, 3, 3)  ...  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.546590  1179648  (512, 256, 3, 3)  ...  2.945027e+22    2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.232446  2359296  (512, 512, 3, 3)  ...  2.945026e+22    1.248265e+16        9.402647e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.232694  2359296  (512, 512, 3, 3)  ...  2.945028e+22    1.248265e+16        9.415211e+31   2.945028e+22      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.211462  2359296  (512, 512, 3, 3)  ...  2.945027e+22    1.248265e+16                 inf   2.945027e+22      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.211558  2359296  (512, 512, 3, 3)  ...  2.945027e+22    1.248265e+16                 inf   2.945027e+22      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.210058  2359296  (512, 512, 3, 3)  ...  2.945028e+22    1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.997656     5120         (10, 512)  ...  2.945027e+22    5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10             (10,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 201358535/313478154 (0.6423)
Testing time: 0.2164442670182325
remaining weights: 
0       1724.000015
1         64.000000
2      36267.000732
3         64.000000
4      71488.001953
5        128.000000
6     138678.996094
7        128.000000
8     260086.007812
9        256.000000
10    452250.000000
11       256.000000
12    452069.015625
13       256.000000
14    644783.976562
15       512.000000
16    548408.988281
17       512.000000
18    548995.007812
19       512.000000
20    498901.992188
21       512.000000
22    499128.011719
23       512.000000
24    495587.988281
25       512.000000
26      5107.999878
27        10.000000
dtype: float64
flop each layer: [1765376.015625, 65536.0, 37137408.75, 65536.0, 18300928.5, 32768.0, 35501823.0, 32768.0, 16645504.5, 16384.0, 28944000.0, 16384.0, 28932417.0, 16384.0, 10316543.625, 8192.0, 8774543.8125, 8192.0, 8783920.125, 8192.0, 1995607.96875, 2048.0, 1996512.046875, 2048.0, 1982351.953125, 2048.0, 5107.9998779296875, 10.0]
compression ratio: 1.0 ::: pruner: randTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.302585          10.00          50.09
Final      200     2.30266   2.302588          10.00          50.00
Prune results:
             module   param  sparsity     size             shape  ...    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.116319     1728     (64, 3, 3, 3)  ...    36.652725        0.815036            0.368591   1.408382e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.099908    36864    (64, 64, 3, 3)  ...   -39.299339        0.796347            0.363453   2.935652e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.101183    73728   (128, 64, 3, 3)  ...    58.265282        0.797163            0.363782   5.877327e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.100749   147456  (128, 128, 3, 3)  ...   674.828247        0.796545            0.362152   1.174553e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.100060   294912  (256, 128, 3, 3)  ...  -820.629761        0.798746            0.363700   2.355599e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.100389   589824  (256, 256, 3, 3)  ...  1077.958740        0.798131            0.364734   4.707570e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.100045   589824  (256, 256, 3, 3)  ...   -96.816536        0.798116            0.362752   4.707481e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.099703  1179648  (512, 256, 3, 3)  ...   159.573608        0.796898            0.362425   9.400594e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.099789  2359296  (512, 512, 3, 3)  ... -2571.204102        0.797929            0.363541   1.882550e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.099903  2359296  (512, 512, 3, 3)  ...   204.444565        0.797193            0.363301   1.880815e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.100319  2359296  (512, 512, 3, 3)  ...  -141.024368        0.798592            0.363517   1.884115e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.099862  2359296  (512, 512, 3, 3)  ... -1582.446777        0.797628            0.362716   1.881840e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.100067  2359296  (512, 512, 3, 3)  ... -1534.578979        0.798339            0.363479   1.883518e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.099609     5120         (10, 512)  ...     2.506975        0.801858            0.361405   4.105513e+03      True
27              fc    bias  1.000000       10             (10,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 1475793/14719818 (0.1003)
FLOP Sparsity: 31675248/313478154 (0.1010)
Testing time: 0.23009926499798894
remaining weights: 
0        201.000006
1         64.000000
2       3683.000061
3         64.000000
4       7460.000244
5        128.000000
6      14855.999634
7        128.000000
8      29509.000488
9        256.000000
10     59212.001953
11       256.000000
12     59009.000977
13       256.000000
14    117614.003906
15       512.000000
16    235431.000000
17       512.000000
18    235699.998047
19       512.000000
20    236682.000000
21       512.000000
22    235604.003906
23       512.000000
24    236088.000000
25       512.000000
26       510.000000
27        10.000000
dtype: float64
flop each layer: [205824.005859375, 65536.0, 3771392.0625, 65536.0, 1909760.0625, 32768.0, 3803135.90625, 32768.0, 1888576.03125, 16384.0, 3789568.125, 16384.0, 3776576.0625, 16384.0, 1881824.0625, 8192.0, 3766896.0, 8192.0, 3771199.96875, 8192.0, 946728.0, 2048.0, 942416.015625, 2048.0, 944352.0, 2048.0, 510.0, 10.0]
compression ratio: 1.0 ::: pruner: snipTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.328871          10.00          50.00
Final      200    0.069111   0.661173          87.79          99.14
Prune results:
             module   param  sparsity     size             shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.890046     1728     (64, 3, 3, 3)  ...   0.003890    2.251246e-06        6.609030e-12       0.003890      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.508979    36864    (64, 64, 3, 3)  ...   0.015803    4.286826e-07        4.759631e-13       0.015803      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.384087    73728   (128, 64, 3, 3)  ...   0.022509    3.053045e-07        3.056363e-13       0.022509      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.254015   147456  (128, 128, 3, 3)  ...   0.027051    1.834511e-07        1.399391e-13       0.027051      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.203756   294912  (256, 128, 3, 3)  ...   0.041614    1.411075e-07        8.782232e-14       0.041614      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.130288   589824  (256, 256, 3, 3)  ...   0.051114    8.665904e-08        3.704082e-14       0.051114      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.149158   589824  (256, 256, 3, 3)  ...   0.057322    9.718497e-08        4.207596e-14       0.057322      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.131023  1179648  (512, 256, 3, 3)  ...   0.099587    8.442076e-08        2.961277e-14       0.099587      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.078696  2359296  (512, 512, 3, 3)  ...   0.128964    5.466201e-08        1.528223e-14       0.128964      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.082224  2359296  (512, 512, 3, 3)  ...   0.133431    5.655530e-08        1.540730e-14       0.133431      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.099551  2359296  (512, 512, 3, 3)  ...   0.149802    6.349430e-08        2.119348e-14       0.149802      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.080867  2359296  (512, 512, 3, 3)  ...   0.125793    5.331815e-08        2.339025e-14       0.125793      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.083440  2359296  (512, 512, 3, 3)  ...   0.130185    5.517954e-08        2.614561e-14       0.130185      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.748047     5120         (10, 512)  ...   0.012935    2.526402e-06        1.791466e-11       0.012935      True
27              fc    bias  1.000000       10             (10,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 1475792/14719818 (0.1003)
FLOP Sparsity: 63338772/313478154 (0.2021)
Testing time: 0.222720076009864
remaining weights: 
0       1538.000004
1         64.000000
2      18763.000488
3         64.000000
4      28317.999023
5        128.000000
6      37456.000488
7        128.000000
8      60089.998535
9        256.000000
10     76847.000977
11       256.000000
12     87977.003906
13       256.000000
14    154561.007812
15       512.000000
16    185666.994141
17       512.000000
18    193990.992188
19       512.000000
20    234870.996094
21       512.000000
22    190789.998047
23       512.000000
24    196860.005859
25       512.000000
26      3830.000000
27        10.000000
dtype: float64
flop each layer: [1574912.00390625, 65536.0, 19213312.5, 65536.0, 7249407.75, 32768.0, 9588736.125, 32768.0, 3845759.90625, 16384.0, 4918208.0625, 16384.0, 5630528.25, 16384.0, 2472976.125, 8192.0, 2970671.90625, 8192.0, 3103855.875, 8192.0, 939483.984375, 2048.0, 763159.9921875, 2048.0, 787440.0234375, 2048.0, 3830.0, 10.0]
compression ratio: 1.0 ::: pruner: graspTrain results:
                 train_loss     test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN  2.417717e+00          11.73          50.17
Pre-Prune  0           NaN  2.417717e+00          11.73          50.17
Post-Prune 0           NaN  9.689741e+06          10.00          50.00
Final      200    0.246969  6.417348e-01          82.15          98.97
Prune results:
             module   param  sparsity     size             shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.516204     1728     (64, 3, 3, 3)  ...   1.037378    4.939478e-03        4.980458e-05       8.535418      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.394450    36864    (64, 64, 3, 3)  ...   0.037528    1.123955e-04        7.802992e-08       4.143348      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.319838    73728   (128, 64, 3, 3)  ...   0.134898    6.910266e-05        4.388734e-08       5.094801      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.232815   147456  (128, 128, 3, 3)  ...  -0.081947    2.642712e-05        1.049846e-08       3.896838      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.198897   294912  (256, 128, 3, 3)  ...   0.086092    1.582900e-05        4.794441e-09       4.668161      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.149192   589824  (256, 256, 3, 3)  ...  -0.084446    5.860383e-06        1.370150e-09       3.456595      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.155457   589824  (256, 256, 3, 3)  ...  -0.032019    4.811619e-06        7.995736e-10       2.838008      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.133161  1179648  (512, 256, 3, 3)  ...  -0.020124    2.211682e-06        2.736238e-10       2.609006      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.084589  2359296  (512, 512, 3, 3)  ...  -0.004147    5.854920e-07        4.381903e-11       1.381349      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.087316  2359296  (512, 512, 3, 3)  ...   0.003847    3.312944e-07        2.752568e-11       0.781622      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.084507  2359296  (512, 512, 3, 3)  ...  -0.012188    1.570893e-07        1.344796e-11       0.370620      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.081262  2359296  (512, 512, 3, 3)  ...  -0.023898    8.280107e-08        2.872616e-11       0.195352      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.086292  2359296  (512, 512, 3, 3)  ...  -0.015937    5.732012e-08        3.739610e-11       0.135235      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.493555     5120         (10, 512)  ...  -0.025036    4.613624e-05        1.461783e-07       0.236218      True
27              fc    bias  1.000000       10             (10,)  ...   0.000000    0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 1475793/14719818 (0.1003)
FLOP Sparsity: 57543020/313478154 (0.1836)
Testing time: 0.22986464700079523
remaining weights: 
0        891.999996
1         64.000000
2      14540.999634
3         64.000000
4      23580.999756
5        128.000000
6      34329.999023
7        128.000000
8      58656.999023
9        256.000000
10     87996.999023
11       256.000000
12     91692.000000
13       256.000000
14    157082.994141
15       512.000000
16    199570.007812
17       512.000000
18    206004.005859
19       512.000000
20    199375.998047
21       512.000000
22    191720.003906
23       512.000000
24    203589.000000
25       512.000000
26      2526.999969
27        10.000000
dtype: float64
flop each layer: [913407.99609375, 65536.0, 14889983.625, 65536.0, 6036735.9375, 32768.0, 8788479.75, 32768.0, 3754047.9375, 16384.0, 5631807.9375, 16384.0, 5868288.0, 16384.0, 2513327.90625, 8192.0, 3193120.125, 8192.0, 3296064.09375, 8192.0, 797503.9921875, 2048.0, 766880.015625, 2048.0, 814356.0, 2048.0, 2526.999969482422, 10.0]
compression ratio: 1.0 ::: pruner: synflowTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.302601          10.00          49.94
Final      200    0.034459   0.689078          88.77          99.06
Prune results:
             module   param  sparsity     size             shape  ...     score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.997106     1728     (64, 3, 3, 3)  ...  2.945027e+22    1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.964790    36864    (64, 64, 3, 3)  ...  2.945027e+22    7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.928589    73728   (128, 64, 3, 3)  ...  2.945027e+22    3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.859741   147456  (128, 128, 3, 3)  ...  2.945027e+22    1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.726661   294912  (256, 128, 3, 3)  ...  2.945027e+22    9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.484931   589824  (256, 256, 3, 3)  ...  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.484745   589824  (256, 256, 3, 3)  ...  2.945027e+22    4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.162064  1179648  (512, 256, 3, 3)  ...  2.945027e+22    2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.007960  2359296  (512, 512, 3, 3)  ...  2.945026e+22    1.248265e+16        9.402647e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.007958  2359296  (512, 512, 3, 3)  ...  2.945028e+22    1.248265e+16        9.415211e+31   2.945028e+22      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.029585  2359296  (512, 512, 3, 3)  ...  2.945027e+22    1.248265e+16                 inf   2.945027e+22      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.029672  2359296  (512, 512, 3, 3)  ...  2.945027e+22    1.248265e+16                 inf   2.945027e+22      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.033553  2359296  (512, 512, 3, 3)  ...  2.945028e+22    1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.994922     5120         (10, 512)  ...  2.945027e+22    5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10             (10,)  ...  0.000000e+00    0.000000e+00        0.000000e+00   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 1475793/14719818 (0.1003)
FLOP Sparsity: 143301085/313478154 (0.4571)
Testing time: 0.21506386599503458
remaining weights: 
0       1723.000019
1         64.000000
2      35566.000488
3         64.000000
4      68463.000000
5        128.000000
6     126774.000000
7        128.000000
8     214301.003906
9        256.000000
10    286024.007812
11       256.000000
12    285914.003906
13       256.000000
14    191178.000000
15       512.000000
16     18779.000977
17       512.000000
18     18775.999512
19       512.000000
20     69800.000977
21       512.000000
22     70005.001465
23       512.000000
24     79161.996094
25       512.000000
26      5093.999939
27        10.000000
dtype: float64
flop each layer: [1764352.01953125, 65536.0, 36419584.5, 65536.0, 17526528.0, 32768.0, 32454144.0, 32768.0, 13715264.25, 16384.0, 18305536.5, 16384.0, 18298496.25, 16384.0, 3058848.0, 8192.0, 300464.015625, 8192.0, 300415.9921875, 8192.0, 279200.00390625, 2048.0, 280020.005859375, 2048.0, 316647.984375, 2048.0, 5093.999938964844, 10.0]
compression ratio: 2.0 ::: pruner: randTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.302585           9.82          49.94
Final      200    2.302657   2.302589          10.00          50.00
Prune results:
             module   param  sparsity     size             shape  ...    score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.008681     1728     (64, 3, 3, 3)  ...   -45.719090        0.783392            0.349013   1.353702e+03      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
2    layers.1.conv  weight  0.010281    36864    (64, 64, 3, 3)  ...   382.588440        0.795274            0.361544   2.931698e+04      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
4    layers.3.conv  weight  0.009942    73728   (128, 64, 3, 3)  ...   353.654938        0.798878            0.359903   5.889966e+04      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
6    layers.4.conv  weight  0.009915   147456  (128, 128, 3, 3)  ...  -412.104187        0.797797            0.361550   1.176400e+05      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
8    layers.6.conv  weight  0.009722   294912  (256, 128, 3, 3)  ...   562.895508        0.797750            0.363807   2.352659e+05      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
10   layers.7.conv  weight  0.009761   589824  (256, 256, 3, 3)  ...   748.252563        0.796868            0.362748   4.700119e+05      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
12   layers.8.conv  weight  0.010091   589824  (256, 256, 3, 3)  ...  -688.653625        0.796721            0.363388   4.699254e+05      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
14  layers.10.conv  weight  0.009952  1179648  (512, 256, 3, 3)  ...  -961.843262        0.798103            0.363234   9.414810e+05      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
16  layers.11.conv  weight  0.010011  2359296  (512, 512, 3, 3)  ... -1023.726379        0.797002            0.362905   1.880363e+06      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
18  layers.12.conv  weight  0.010125  2359296  (512, 512, 3, 3)  ...  2036.284058        0.797733            0.363538   1.882089e+06      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
20  layers.14.conv  weight  0.010034  2359296  (512, 512, 3, 3)  ...  2780.933350        0.798444            0.363815   1.883767e+06      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
22  layers.15.conv  weight  0.010022  2359296  (512, 512, 3, 3)  ...    93.093445        0.798203            0.364225   1.883198e+06      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
24  layers.16.conv  weight  0.009911  2359296  (512, 512, 3, 3)  ... -3537.218750        0.797848            0.363162   1.882360e+06      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False
26              fc  weight  0.007812     5120         (10, 512)  ...   -96.056976        0.786234            0.362159   4.025518e+03      True
27              fc    bias  1.000000       10             (10,)  ...     0.000000        0.000000            0.000000   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 151389/14719818 (0.0103)
FLOP Sparsity: 3405561/313478154 (0.0109)
Testing time: 0.23607753100804985
remaining weights: 
0        15.000001
1        64.000000
2       378.999996
3        64.000000
4       732.999985
5       128.000000
6      1461.999985
7       128.000000
8      2866.999878
9       256.000000
10     5757.000183
11      256.000000
12     5951.999817
13      256.000000
14    11740.000122
15      512.000000
16    23619.999023
17      512.000000
18    23888.999268
19      512.000000
20    23673.999023
21      512.000000
22    23644.999512
23      512.000000
24    23382.999756
25      512.000000
26       40.000000
27       10.000000
dtype: float64
flop each layer: [15360.000732421875, 65536.0, 388095.99609375, 65536.0, 187647.99609375, 32768.0, 374271.99609375, 32768.0, 183487.9921875, 16384.0, 368448.01171875, 16384.0, 380927.98828125, 16384.0, 187840.001953125, 8192.0, 377919.984375, 8192.0, 382223.98828125, 8192.0, 94695.99609375, 2048.0, 94579.998046875, 2048.0, 93531.9990234375, 2048.0, 40.0, 10.0]
compression ratio: 2.0 ::: pruner: snipTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.302598          11.87          50.00
Final      200    0.322838   0.510305          84.05          99.13
Prune results:
             module   param  sparsity     size  ... score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.604745     1728  ...   2.197405e-06        7.518065e-12       0.003797      True
1    layers.0.conv    bias  1.000000       64  ...   0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.136095    36864  ...   3.936874e-07        4.314465e-13       0.014513      True
3    layers.1.conv    bias  1.000000       64  ...   0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.106635    73728  ...   3.099517e-07        3.070988e-13       0.022852      True
5    layers.3.conv    bias  1.000000      128  ...   0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.053758   147456  ...   1.886923e-07        1.548731e-13       0.027824      True
7    layers.4.conv    bias  1.000000      128  ...   0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.033332   294912  ...   1.367737e-07        8.790606e-14       0.040336      True
9    layers.6.conv    bias  1.000000      256  ...   0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.016222   589824  ...   9.050445e-08        4.122895e-14       0.053382      True
11   layers.7.conv    bias  1.000000      256  ...   0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.017281   589824  ...   9.927916e-08        4.589619e-14       0.058557      True
13   layers.8.conv    bias  1.000000      256  ...   0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.012492  1179648  ...   8.710396e-08        3.302739e-14       0.102752      True
15  layers.10.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.005281  2359296  ...   5.618509e-08        1.671744e-14       0.132557      True
17  layers.11.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.004626  2359296  ...   5.629265e-08        1.528436e-14       0.132811      True
19  layers.12.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.007157  2359296  ...   6.378662e-08        2.107211e-14       0.150492      True
21  layers.14.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.007715  2359296  ...   5.152193e-08        2.194914e-14       0.121555      True
23  layers.15.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.008414  2359296  ...   5.341541e-08        2.443162e-14       0.126023      True
25  layers.16.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.520898     5120  ...   2.450962e-06        1.614598e-11       0.012549      True
27              fc    bias  1.000000       10  ...   0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 151390/14719818 (0.0103)
FLOP Sparsity: 13251949/313478154 (0.0423)
Testing time: 0.21714872799930163
remaining weights: 
0      1045.000031
1        64.000000
2      5017.000122
3        64.000000
4      7861.999878
5       128.000000
6      7926.999939
7       128.000000
8      9829.999512
9       256.000000
10     9567.999756
11      256.000000
12    10192.999878
13      256.000000
14    14736.000366
15      512.000000
16    12460.000122
17      512.000000
18    10912.999878
19      512.000000
20    16884.999756
21      512.000000
22    18202.999878
23      512.000000
24    19850.000977
25      512.000000
26     2667.000122
27       10.000000
dtype: float64
flop each layer: [1070080.03125, 65536.0, 5137408.125, 65536.0, 2012671.96875, 32768.0, 2029311.984375, 32768.0, 629119.96875, 16384.0, 612351.984375, 16384.0, 652351.9921875, 16384.0, 235776.005859375, 8192.0, 199360.001953125, 8192.0, 174607.998046875, 8192.0, 67539.9990234375, 2048.0, 72811.99951171875, 2048.0, 79400.00390625, 2048.0, 2667.0001220703125, 10.0]
compression ratio: 2.0 ::: pruner: graspTrain results:
                 train_loss   test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN    2.417717          11.73          50.17
Pre-Prune  0           NaN    2.417717          11.73          50.17
Post-Prune 0           NaN  454.316233          10.00          50.00
Final      200    0.342315    0.499896          83.66          99.29
Prune results:
             module   param  sparsity     size  ... score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.454861     1728  ...   2.025770e-02        1.358055e-03      35.005314      True
1    layers.0.conv    bias  1.000000       64  ...   0.000000e+00        0.000000e+00       0.000000     False
2    layers.1.conv  weight  0.195584    36864  ...   4.721189e-04        3.802939e-06      17.404190      True
3    layers.1.conv    bias  1.000000       64  ...   0.000000e+00        0.000000e+00       0.000000     False
4    layers.3.conv  weight  0.142958    73728  ...   2.425217e-04        1.551742e-06      17.880636      True
5    layers.3.conv    bias  1.000000      128  ...   0.000000e+00        0.000000e+00       0.000000     False
6    layers.4.conv  weight  0.082499   147456  ...   7.997586e-05        3.918720e-07      11.792920      True
7    layers.4.conv    bias  1.000000      128  ...   0.000000e+00        0.000000e+00       0.000000     False
8    layers.6.conv  weight  0.058309   294912  ...   4.069104e-05        1.583985e-07      12.000275      True
9    layers.6.conv    bias  1.000000      256  ...   0.000000e+00        0.000000e+00       0.000000     False
10   layers.7.conv  weight  0.030884   589824  ...   1.362163e-05        4.099782e-08       8.034362      True
11   layers.7.conv    bias  1.000000      256  ...   0.000000e+00        0.000000e+00       0.000000     False
12   layers.8.conv  weight  0.024662   589824  ...   1.061060e-05        2.441220e-08       6.258387      True
13   layers.8.conv    bias  1.000000      256  ...   0.000000e+00        0.000000e+00       0.000000     False
14  layers.10.conv  weight  0.013835  1179648  ...   4.590902e-06        7.017222e-09       5.415648      True
15  layers.10.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00       0.000000     False
16  layers.11.conv  weight  0.003998  2359296  ...   1.221636e-06        1.464409e-09       2.882200      True
17  layers.11.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00       0.000000     False
18  layers.12.conv  weight  0.003143  2359296  ...   9.887677e-07        1.298216e-09       2.332796      True
19  layers.12.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00       0.000000     False
20  layers.14.conv  weight  0.003503  2359296  ...   7.779260e-07        7.131838e-10       1.835358      True
21  layers.14.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00       0.000000     False
22  layers.15.conv  weight  0.004338  2359296  ...   5.435103e-07        4.193051e-10       1.282302      True
23  layers.15.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00       0.000000     False
24  layers.16.conv  weight  0.005548  2359296  ...   5.084717e-07        2.482806e-10       1.199635      True
25  layers.16.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00       0.000000     False
26              fc  weight  0.340039     5120  ...   2.274245e-04        8.917817e-07       1.164413      True
27              fc    bias  1.000000       10  ...   0.000000e+00        0.000000e+00       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 151389/14719818 (0.0103)
FLOP Sparsity: 18132954/313478154 (0.0578)
Testing time: 0.22079130198108032
remaining weights: 
0       785.999989
1        64.000000
2      7209.999756
3        64.000000
4     10539.999756
5       128.000000
6     12164.999634
7       128.000000
8     17195.999634
9       256.000000
10    18216.000000
11      256.000000
12    14545.999512
13      256.000000
14    16320.000366
15      512.000000
16     9432.000000
17      512.000000
18     7415.000244
19      512.000000
20     8264.999817
21      512.000000
22    10234.999512
23      512.000000
24    13089.000366
25      512.000000
26     1741.000061
27       10.000000
dtype: float64
flop each layer: [804863.98828125, 65536.0, 7383039.75, 65536.0, 2698239.9375, 32768.0, 3114239.90625, 32768.0, 1100543.9765625, 16384.0, 1165824.0, 16384.0, 930943.96875, 16384.0, 261120.005859375, 8192.0, 150912.0, 8192.0, 118640.00390625, 8192.0, 33059.999267578125, 2048.0, 40939.998046875, 2048.0, 52356.00146484375, 2048.0, 1741.0000610351562, 10.0]
compression ratio: 2.0 ::: pruner: synflowTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  0           NaN   2.417717          11.73          50.17
Post-Prune 0           NaN   2.302585          10.00          50.00
Final      200    2.302653   2.302588          10.00          50.00
Prune results:
             module   param  sparsity     size  ... score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.988426     1728  ...   1.704298e+19                 inf   2.945027e+22      True
1    layers.0.conv    bias  1.000000       64  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
2    layers.1.conv  weight  0.806396    36864  ...   7.988898e+17                 inf   2.945027e+22      True
3    layers.1.conv    bias  1.000000       64  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
4    layers.3.conv  weight  0.626017    73728  ...   3.994448e+17                 inf   2.945027e+22      True
5    layers.3.conv    bias  1.000000      128  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
6    layers.4.conv  weight  0.332425   147456  ...   1.997224e+17                 inf   2.945027e+22      True
7    layers.4.conv    bias  1.000000      128  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
8    layers.6.conv  weight  0.052199   294912  ...   9.986122e+16                 inf   2.945027e+22      True
9    layers.6.conv    bias  1.000000      256  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
10   layers.7.conv  weight  0.000154   589824  ...   4.993061e+16                 inf   2.945027e+22      True
11   layers.7.conv    bias  1.000000      256  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
12   layers.8.conv  weight  0.000112   589824  ...   4.993061e+16                 inf   2.945027e+22      True
13   layers.8.conv    bias  1.000000      256  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
14  layers.10.conv  weight  0.000000  1179648  ...   2.496530e+16                 inf   2.945027e+22      True
15  layers.10.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
16  layers.11.conv  weight  0.000000  2359296  ...   1.248265e+16        9.402647e+31   2.945026e+22      True
17  layers.11.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
18  layers.12.conv  weight  0.000000  2359296  ...   1.248265e+16        9.415211e+31   2.945028e+22      True
19  layers.12.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
20  layers.14.conv  weight  0.000000  2359296  ...   1.248265e+16                 inf   2.945027e+22      True
21  layers.14.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
22  layers.15.conv  weight  0.000000  2359296  ...   1.248265e+16                 inf   2.945027e+22      True
23  layers.15.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
24  layers.16.conv  weight  0.000000  2359296  ...   1.248265e+16                 inf   2.945028e+22      True
25  layers.16.conv    bias  1.000000      512  ...   0.000000e+00        0.000000e+00   0.000000e+00     False
26              fc  weight  0.975977     5120  ...   5.752007e+18                 inf   2.945027e+22      True
27              fc    bias  1.000000       10  ...   0.000000e+00        0.000000e+00   0.000000e+00     False

[28 rows x 13 columns]
Parameter Sparsity: 151390/14719818 (0.0103)
FLOP Sparsity: 57830479/313478154 (0.1845)
Testing time: 0.21764679899206385
remaining weights: 
0      1707.999973
1        64.000000
2     29727.000000
3        64.000000
4     46155.001465
5       128.000000
6     49017.999023
7       128.000000
8     15394.000122
9       256.000000
10       90.999996
11      256.000000
12       66.000001
13      256.000000
14        0.000000
15      512.000000
16        0.000000
17      512.000000
18        0.000000
19      512.000000
20        0.000000
21      512.000000
22        0.000000
23      512.000000
24        0.000000
25      512.000000
26     4997.000122
27       10.000000
dtype: float64
flop each layer: [1748991.97265625, 65536.0, 30440448.0, 65536.0, 11815680.375, 32768.0, 12548607.75, 32768.0, 985216.0078125, 16384.0, 5823.999755859375, 16384.0, 4224.000091552734, 16384.0, 0.0, 8192.0, 0.0, 8192.0, 0.0, 8192.0, 0.0, 2048.0, 0.0, 2048.0, 0.0, 2048.0, 4997.0001220703125, 10.0]
