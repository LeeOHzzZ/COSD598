start!
compression ratio: 0.5 ::: pruner: rand
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.306855          10.32          47.39
Pre-Prune  0           NaN   2.306855          10.32          47.39
Post-Prune 0           NaN   2.305102          10.32          50.29
Final      200    0.000003   0.256418          97.63          99.93
Prune results:
    module   param  sparsity   size       shape  ...   score sum  score abs mean  score abs variance  score abs sum  prunable
0       1  weight  0.317895  78400  (100, 784)  ...   77.354195        0.798343            0.363375   62590.125000      True
1       1    bias  1.000000    100      (100,)  ...    0.000000        0.000000            0.000000       0.000000     False
2       3  weight  0.308800  10000  (100, 100)  ...  -41.945251        0.792052            0.367775    7920.516602      True
3       3    bias  1.000000    100      (100,)  ...    0.000000        0.000000            0.000000       0.000000     False
4       5  weight  0.317300  10000  (100, 100)  ...  100.989563        0.789619            0.352073    7896.190430      True
5       5    bias  1.000000    100      (100,)  ...    0.000000        0.000000            0.000000       0.000000     False
6       7  weight  0.314100  10000  (100, 100)  ...  -25.344994        0.791694            0.361521    7916.939453      True
7       7    bias  1.000000    100      (100,)  ...    0.000000        0.000000            0.000000       0.000000     False
8       9  weight  0.310300  10000  (100, 100)  ... -122.213715        0.792565            0.354341    7925.648438      True
9       9    bias  1.000000    100      (100,)  ...    0.000000        0.000000            0.000000       0.000000     False
10     11  weight  0.330000   1000   (10, 100)  ...  -24.212666        0.826184            0.353799     826.184204      True
11     11    bias  1.000000     10       (10,)  ...    0.000000        0.000000            0.000000       0.000000     False

[12 rows x 13 columns]
Parameter Sparsity: 38268/119910 (0.3191)
FLOP Sparsity: 38268/119910 (0.3191)
Testing time: 0.17697620500257472
remaining weights: 
0     24923.000336
1       100.000000
2      3088.000119
3       100.000000
4      3172.999918
5       100.000000
6      3140.999973
7       100.000000
8      3102.999926
9       100.000000
10      330.000013
11       10.000000
dtype: float64
flop each layer: [24923.00033569336, 100.0, 3088.0001187324524, 100.0, 3172.999918460846, 100.0, 3140.999972820282, 100.0, 3102.9999256134033, 100.0, 330.00001311302185, 10.0]


compression ratio: 0.5 ::: pruner: snip
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.306855          10.32          47.39
Pre-Prune  0           NaN   2.306855          10.32          47.39
Post-Prune 0           NaN   2.307302          10.32          49.08
Final      200    0.000005   0.204932          97.86          99.87
Prune results:
    module   param  sparsity   size       shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0       1  weight  0.291696  78400  (100, 784)  ...   0.404448        0.000005        5.989315e-11       0.404448      True
1       1    bias  1.000000    100      (100,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
2       3  weight  0.485500  10000  (100, 100)  ...   0.125934        0.000013        4.284688e-10       0.125934      True
3       3    bias  1.000000    100      (100,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
4       5  weight  0.388500  10000  (100, 100)  ...   0.111812        0.000011        4.830765e-10       0.111812      True
5       5    bias  1.000000    100      (100,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
6       7  weight  0.302600  10000  (100, 100)  ...   0.103630        0.000010        6.846701e-10       0.103630      True
7       7    bias  1.000000    100      (100,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
8       9  weight  0.263200  10000  (100, 100)  ...   0.158670        0.000016        2.011727e-09       0.158670      True
9       9    bias  1.000000    100      (100,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
10     11  weight  0.491000   1000   (10, 100)  ...   0.095505        0.000096        5.414625e-08       0.095505      True
11     11    bias  1.000000     10       (10,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False

[12 rows x 13 columns]
Parameter Sparsity: 38268/119910 (0.3191)
FLOP Sparsity: 38268/119910 (0.3191)
Testing time: 0.17615519899845822
remaining weights: 
0     22869.000053
1       100.000000
2      4855.000079
3       100.000000
4      3885.000050
5       100.000000
6      3025.999963
7       100.000000
8      2632.000148
9       100.000000
10      490.999997
11       10.000000
dtype: float64
flop each layer: [22869.00005340576, 100.0, 4855.000078678131, 100.0, 3885.0000500679016, 100.0, 3025.999963283539, 100.0, 2632.000148296356, 100.0, 490.9999966621399, 10.0]


compression ratio: 0.5 ::: pruner: grasp
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.306855          10.32          47.39
Pre-Prune  0           NaN   2.306855          10.32          47.39
Post-Prune 0           NaN   2.309332          10.32          48.19
Final      200    0.000019   0.213377          97.63          99.88
Prune results:
    module   param  sparsity   size       shape  ...  score sum  score abs mean  score abs variance  score abs sum  prunable
0       1  weight   0.32324  78400  (100, 784)  ...  -0.062145        0.000185        7.578730e-07      14.507076      True
1       1    bias   1.00000    100      (100,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
2       3  weight   0.40390  10000  (100, 100)  ...  -0.286799        0.001435        4.160676e-05      14.354935      True
3       3    bias   1.00000    100      (100,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
4       5  weight   0.34870  10000  (100, 100)  ...  -0.127696        0.000702        8.901418e-06       7.018160      True
5       5    bias   1.00000    100      (100,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
6       7  weight   0.26950  10000  (100, 100)  ...  -0.290900        0.000398        3.669886e-06       3.982991      True
7       7    bias   1.00000    100      (100,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
8       9  weight   0.18890  10000  (100, 100)  ...  -0.228726        0.000370        2.755434e-06       3.695314      True
9       9    bias   1.00000    100      (100,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False
10     11  weight   0.30600   1000   (10, 100)  ...  -0.003720        0.002053        5.978542e-05       2.052559      True
11     11    bias   1.00000     10       (10,)  ...   0.000000        0.000000        0.000000e+00       0.000000     False

[12 rows x 13 columns]
Parameter Sparsity: 38268/119910 (0.3191)
FLOP Sparsity: 38268/119910 (0.3191)
Testing time: 0.17705499599833274
remaining weights: 
0     25342.000580
1       100.000000
2      4038.999975
3       100.000000
4      3486.999869
5       100.000000
6      2694.999874
7       100.000000
8      1888.999939
9       100.000000
10      305.999994
11       10.000000
dtype: float64
flop each layer: [25342.000579833984, 100.0, 4038.9999747276306, 100.0, 3486.9998693466187, 100.0, 2694.999873638153, 100.0, 1888.9999389648438, 100.0, 305.9999942779541, 10.0]


compression ratio: 0.5 ::: pruner: synflow
Train results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.306855          10.32          47.39
Pre-Prune  0           NaN   2.306855          10.32          47.39
Post-Prune 0           NaN   2.306739          10.32          49.80
Final      200    0.000015   0.275917          96.90          99.87
Prune results:
    module   param  sparsity   size       shape  ...     score sum  score abs mean  score abs variance  score abs sum  prunable
0       1  weight  0.024209  78400  (100, 784)  ...  421432.43750        5.375414            9.872970   421432.43750      True
1       1    bias  1.000000    100      (100,)  ...       0.00000        0.000000            0.000000        0.00000     False
2       3  weight  0.874200  10000  (100, 100)  ...  421973.71875       42.197372          606.436279   421973.71875      True
3       3    bias  1.000000    100      (100,)  ...       0.00000        0.000000            0.000000        0.00000     False
4       5  weight  0.876000  10000  (100, 100)  ...  422284.00000       42.228401          605.871582   422284.00000      True
5       5    bias  1.000000    100      (100,)  ...       0.00000        0.000000            0.000000        0.00000     False
6       7  weight  0.868400  10000  (100, 100)  ...  422344.31250       42.234432          616.338806   422344.31250      True
7       7    bias  1.000000    100      (100,)  ...       0.00000        0.000000            0.000000        0.00000     False
8       9  weight  0.868300  10000  (100, 100)  ...  422357.31250       42.235733          690.546692   422357.31250      True
9       9    bias  1.000000    100      (100,)  ...       0.00000        0.000000            0.000000        0.00000     False
10     11  weight  0.991000   1000   (10, 100)  ...  422359.28125      422.359283        62346.089844   422359.28125      True
11     11    bias  1.000000     10       (10,)  ...       0.00000        0.000000            0.000000        0.00000     False

[12 rows x 13 columns]
Parameter Sparsity: 38267/119910 (0.3191)
FLOP Sparsity: 38267/119910 (0.3191)
Testing time: 0.17452818500169087
remaining weights: 
0     1898.000070
1      100.000000
2     8741.999865
3      100.000000
4     8759.999871
5      100.000000
6     8683.999777
7      100.000000
8     8683.000207
9      100.000000
10     990.999997
11      10.000000
dtype: float64
flop each layer: [1898.0000704526901, 100.0, 8741.999864578247, 100.0, 8759.999871253967, 100.0, 8683.99977684021, 100.0, 8683.000206947327, 100.0, 990.9999966621399, 10.0]
