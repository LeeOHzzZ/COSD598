start!
compression ratio: 0.05 ::: pruner: magTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  200    0.397699   0.572066          86.12          98.46
Post-Prune 0           NaN   0.569941          86.12          98.42
Final      200    0.062494   0.736970          89.48          98.36
Prune results:
             module   param  sparsity     size             shape  ...      score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.986690     1728     (64, 3, 3, 3)  ...     403.510468        0.233513            0.030640     403.510468      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
2    layers.1.conv  weight  0.970459    36864    (64, 64, 3, 3)  ...    5341.592773        0.144900            0.014448    5341.592773      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
4    layers.3.conv  weight  0.972616    73728   (128, 64, 3, 3)  ...   11132.814453        0.150998            0.015261   11132.814453      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
6    layers.4.conv  weight  0.968397   147456  (128, 128, 3, 3)  ...   20848.654297        0.141389            0.013431   20848.654297      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
8    layers.6.conv  weight  0.967516   294912  (256, 128, 3, 3)  ...   41785.378906        0.141688            0.013859   41785.378906      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
10   layers.7.conv  weight  0.968206   589824  (256, 256, 3, 3)  ...   85015.125000        0.144136            0.013674   85015.125000      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
12   layers.8.conv  weight  0.965829   589824  (256, 256, 3, 3)  ...   82092.585938        0.139181            0.012897   82092.585938      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
14  layers.10.conv  weight  0.971223  1179648  (512, 256, 3, 3)  ...  165819.281250        0.140567            0.011844  165819.281250      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
16  layers.11.conv  weight  0.958758  2359296  (512, 512, 3, 3)  ...  264392.937500        0.112064            0.008583  264392.937500      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
18  layers.12.conv  weight  0.902525  2359296  (512, 512, 3, 3)  ...  138777.953125        0.058822            0.003802  138777.953125      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
20  layers.14.conv  weight  0.842495  2359296  (512, 512, 3, 3)  ...   74926.140625        0.031758            0.001230   74926.140625      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
22  layers.15.conv  weight  0.819450  2359296  (512, 512, 3, 3)  ...   59823.328125        0.025356            0.000711   59823.328125      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
24  layers.16.conv  weight  0.836859  2359296  (512, 512, 3, 3)  ...   70122.359375        0.029722            0.001017   70122.359375      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
26              fc  weight  0.934766     5120         (10, 512)  ...     430.445740        0.084071            0.008304     430.445740      True
27              fc    bias  1.000000       10             (10,)  ...       0.000000        0.000000            0.000000       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 13119512/14719818 (0.8913)
FLOP Sparsity: 297016642/313478154 (0.9475)
Testing time: 0.21053902700077742
remaining weights: 
0     1.705000e+03
1     6.400000e+01
2     3.577500e+04
3     6.400000e+01
4     7.170900e+04
5     1.280000e+02
6     1.427960e+05
7     1.280000e+02
8     2.853320e+05
9     2.560000e+02
10    5.710710e+05
11    2.560000e+02
12    5.696690e+05
13    2.560000e+02
14    1.145701e+06
15    5.120000e+02
16    2.261994e+06
17    5.120000e+02
18    2.129323e+06
19    5.120000e+02
20    1.987695e+06
21    5.120000e+02
22    1.933326e+06
23    5.120000e+02
24    1.974397e+06
25    5.120000e+02
26    4.786000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1745919.984375, 65536.0, 36633600.0, 65536.0, 18357503.625, 32768.0, 36555777.0, 32768.0, 18261247.5, 16384.0, 36548543.25, 16384.0, 36458815.5, 16384.0, 18331215.75, 8192.0, 36191904.75, 8192.0, 34069167.0, 8192.0, 7950780.0, 2048.0, 7733304.0, 2048.0, 7897587.75, 2048.0, 4786.000061035156, 10.0]
compression ratio: 0.1 ::: pruner: magTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  200    0.397699   0.572066          86.12          98.46
Post-Prune 0           NaN   0.577418          86.22          98.38
Final      200    0.028691   0.765520          89.87          98.52
Prune results:
             module   param  sparsity     size             shape  ...      score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.969329     1728     (64, 3, 3, 3)  ...     403.510468        0.233513            0.030640     403.510468      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
2    layers.1.conv  weight  0.943821    36864    (64, 64, 3, 3)  ...    5341.592773        0.144900            0.014448    5341.592773      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
4    layers.3.conv  weight  0.947347    73728   (128, 64, 3, 3)  ...   11132.814453        0.150998            0.015261   11132.814453      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
6    layers.4.conv  weight  0.939487   147456  (128, 128, 3, 3)  ...   20848.654297        0.141389            0.013431   20848.654297      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
8    layers.6.conv  weight  0.937829   294912  (256, 128, 3, 3)  ...   41785.378906        0.141688            0.013859   41785.378906      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
10   layers.7.conv  weight  0.938431   589824  (256, 256, 3, 3)  ...   85015.125000        0.144136            0.013674   85015.125000      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
12   layers.8.conv  weight  0.934204   589824  (256, 256, 3, 3)  ...   82092.585938        0.139181            0.012897   82092.585938      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
14  layers.10.conv  weight  0.944425  1179648  (512, 256, 3, 3)  ...  165819.281250        0.140567            0.011844  165819.281250      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
16  layers.11.conv  weight  0.920460  2359296  (512, 512, 3, 3)  ...  264392.937500        0.112064            0.008583  264392.937500      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
18  layers.12.conv  weight  0.814342  2359296  (512, 512, 3, 3)  ...  138777.953125        0.058822            0.003802  138777.953125      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
20  layers.14.conv  weight  0.702844  2359296  (512, 512, 3, 3)  ...   74926.140625        0.031758            0.001230   74926.140625      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
22  layers.15.conv  weight  0.661165  2359296  (512, 512, 3, 3)  ...   59823.328125        0.025356            0.000711   59823.328125      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
24  layers.16.conv  weight  0.692350  2359296  (512, 512, 3, 3)  ...   70122.359375        0.029722            0.001017   70122.359375      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
26              fc  weight  0.877734     5120         (10, 512)  ...     430.445740        0.084071            0.008304     430.445740      True
27              fc    bias  1.000000       10             (10,)  ...       0.000000        0.000000            0.000000       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 11693237/14719818 (0.7944)
FLOP Sparsity: 282078022/313478154 (0.8998)
Testing time: 0.22576589600066654
remaining weights: 
0     1.675000e+03
1     6.400000e+01
2     3.479300e+04
3     6.400000e+01
4     6.984600e+04
5     1.280000e+02
6     1.385330e+05
7     1.280000e+02
8     2.765770e+05
9     2.560000e+02
10    5.535090e+05
11    2.560000e+02
12    5.510160e+05
13    2.560000e+02
14    1.114089e+06
15    5.120000e+02
16    2.171638e+06
17    5.120000e+02
18    1.921274e+06
19    5.120000e+02
20    1.658217e+06
21    5.120000e+02
22    1.559884e+06
23    5.120000e+02
24    1.633459e+06
25    5.120000e+02
26    4.494000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1715199.99609375, 65536.0, 35628032.25, 65536.0, 17880575.625, 32768.0, 35464448.25, 32768.0, 17700927.75, 16384.0, 35424576.0, 16384.0, 35265024.0, 16384.0, 17825423.625, 8192.0, 34746207.75, 8192.0, 30740384.25, 8192.0, 6632867.8125, 2048.0, 6239535.75, 2048.0, 6533835.75, 2048.0, 4493.999938964844, 10.0]
compression ratio: 0.2 ::: pruner: magTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  200    0.397699   0.572066          86.12          98.46
Post-Prune 0           NaN   0.593392          85.26          98.38
Final      200    0.016991   0.823015          89.97          98.46
Prune results:
             module   param  sparsity     size             shape  ...      score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.934606     1728     (64, 3, 3, 3)  ...     403.510468        0.233513            0.030640     403.510468      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
2    layers.1.conv  weight  0.890788    36864    (64, 64, 3, 3)  ...    5341.592773        0.144900            0.014448    5341.592773      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
4    layers.3.conv  weight  0.896525    73728   (128, 64, 3, 3)  ...   11132.814453        0.150998            0.015261   11132.814453      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
6    layers.4.conv  weight  0.883267   147456  (128, 128, 3, 3)  ...   20848.654297        0.141389            0.013431   20848.654297      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
8    layers.6.conv  weight  0.879066   294912  (256, 128, 3, 3)  ...   41785.378906        0.141688            0.013859   41785.378906      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
10   layers.7.conv  weight  0.881187   589824  (256, 256, 3, 3)  ...   85015.125000        0.144136            0.013674   85015.125000      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
12   layers.8.conv  weight  0.873759   589824  (256, 256, 3, 3)  ...   82092.585938        0.139181            0.012897   82092.585938      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
14  layers.10.conv  weight  0.892060  1179648  (512, 256, 3, 3)  ...  165819.281250        0.140567            0.011844  165819.281250      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
16  layers.11.conv  weight  0.848338  2359296  (512, 512, 3, 3)  ...  264392.937500        0.112064            0.008583  264392.937500      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
18  layers.12.conv  weight  0.659977  2359296  (512, 512, 3, 3)  ...  138777.953125        0.058822            0.003802  138777.953125      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
20  layers.14.conv  weight  0.472575  2359296  (512, 512, 3, 3)  ...   74926.140625        0.031758            0.001230   74926.140625      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
22  layers.15.conv  weight  0.405142  2359296  (512, 512, 3, 3)  ...   59823.328125        0.025356            0.000711   59823.328125      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
24  layers.16.conv  weight  0.455276  2359296  (512, 512, 3, 3)  ...   70122.359375        0.029722            0.001017   70122.359375      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
26              fc  weight  0.772070     5120         (10, 512)  ...     430.445740        0.084071            0.008304     430.445740      True
27              fc    bias  1.000000       10             (10,)  ...       0.000000        0.000000            0.000000       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 9289140/14719818 (0.6311)
FLOP Sparsity: 255016409/313478154 (0.8135)
Testing time: 0.22030797200568486
remaining weights: 
0     1.615000e+03
1     6.400000e+01
2     3.283800e+04
3     6.400000e+01
4     6.609900e+04
5     1.280000e+02
6     1.302430e+05
7     1.280000e+02
8     2.592470e+05
9     2.560000e+02
10    5.197450e+05
11    2.560000e+02
12    5.153640e+05
13    2.560000e+02
14    1.052317e+06
15    5.120000e+02
16    2.001480e+06
17    5.120000e+02
18    1.557082e+06
19    5.120000e+02
20    1.114944e+06
21    5.120000e+02
22    9.558490e+05
23    5.120000e+02
24    1.074130e+06
25    5.120000e+02
26    3.953000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1653760.01953125, 65536.0, 33626112.75, 65536.0, 16921344.375, 32768.0, 33342207.75, 32768.0, 16591808.25, 16384.0, 33263680.5, 16384.0, 32983296.75, 16384.0, 16837071.75, 8192.0, 32023680.75, 8192.0, 24913311.75, 8192.0, 4459775.90625, 2048.0, 3823396.03125, 2048.0, 4296520.125, 2048.0, 3952.9998779296875, 10.0]
compression ratio: 0.5 ::: pruner: magTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  200    0.397699   0.572066          86.12          98.46
Post-Prune 0           NaN   4.635043          37.23          75.36
Final      200    0.015808   1.004942          90.04          98.74
Prune results:
             module   param  sparsity     size             shape  ...      score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.808449     1728     (64, 3, 3, 3)  ...     403.510468        0.233513            0.030640     403.510468      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
2    layers.1.conv  weight  0.683160    36864    (64, 64, 3, 3)  ...    5341.592773        0.144900            0.014448    5341.592773      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
4    layers.3.conv  weight  0.697388    73728   (128, 64, 3, 3)  ...   11132.814453        0.150998            0.015261   11132.814453      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
6    layers.4.conv  weight  0.667046   147456  (128, 128, 3, 3)  ...   20848.654297        0.141389            0.013431   20848.654297      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
8    layers.6.conv  weight  0.659702   294912  (256, 128, 3, 3)  ...   41785.378906        0.141688            0.013859   41785.378906      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
10   layers.7.conv  weight  0.671405   589824  (256, 256, 3, 3)  ...   85015.125000        0.144136            0.013674   85015.125000      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
12   layers.8.conv  weight  0.657998   589824  (256, 256, 3, 3)  ...   82092.585938        0.139181            0.012897   82092.585938      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
14  layers.10.conv  weight  0.685254  1179648  (512, 256, 3, 3)  ...  165819.281250        0.140567            0.011844  165819.281250      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
16  layers.11.conv  weight  0.591435  2359296  (512, 512, 3, 3)  ...  264392.937500        0.112064            0.008583  264392.937500      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
18  layers.12.conv  weight  0.296635  2359296  (512, 512, 3, 3)  ...  138777.953125        0.058822            0.003802  138777.953125      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
20  layers.14.conv  weight  0.104627  2359296  (512, 512, 3, 3)  ...   74926.140625        0.031758            0.001230   74926.140625      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
22  layers.15.conv  weight  0.056661  2359296  (512, 512, 3, 3)  ...   59823.328125        0.025356            0.000711   59823.328125      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
24  layers.16.conv  weight  0.089959  2359296  (512, 512, 3, 3)  ...   70122.359375        0.029722            0.001017   70122.359375      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
26              fc  weight  0.411914     5120         (10, 512)  ...     430.445740        0.084071            0.008304     430.445740      True
27              fc    bias  1.000000       10             (10,)  ...       0.000000        0.000000            0.000000       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 4657710/14719818 (0.3164)
FLOP Sparsity: 177303509/313478154 (0.5656)
Testing time: 0.21798811698681675
remaining weights: 
0     1.397000e+03
1     6.400000e+01
2     2.518400e+04
3     6.400000e+01
4     5.141700e+04
5     1.280000e+02
6     9.836000e+04
7     1.280000e+02
8     1.945540e+05
9     2.560000e+02
10    3.960110e+05
11    2.560000e+02
12    3.881030e+05
13    2.560000e+02
14    8.083580e+05
15    5.120000e+02
16    1.395371e+06
17    5.120000e+02
18    6.998490e+05
19    5.120000e+02
20    2.468450e+05
21    5.120000e+02
22    1.336800e+05
23    5.120000e+02
24    2.122390e+05
25    5.120000e+02
26    2.109000e+03
27    1.000000e+01
dtype: float64
flop each layer: [1430528.02734375, 65536.0, 25788415.5, 65536.0, 13162752.0, 32768.0, 25180159.5, 32768.0, 12451456.125, 16384.0, 25344704.25, 16384.0, 24838591.5, 16384.0, 12933727.875, 8192.0, 22325935.5, 8192.0, 11197584.0, 8192.0, 987380.015625, 2048.0, 534719.98828125, 2048.0, 848956.0078125, 2048.0, 2108.9999389648438, 10.0]
compression ratio: 1.0 ::: pruner: magTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  200    0.397699   0.572066          86.12          98.46
Post-Prune 0           NaN   9.205447           9.18          49.76
Final      200    0.016444   0.756125          90.00          99.09
Prune results:
             module   param  sparsity     size             shape  ...      score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.534722     1728     (64, 3, 3, 3)  ...     403.510468        0.233513            0.030640     403.510468      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
2    layers.1.conv  weight  0.293213    36864    (64, 64, 3, 3)  ...    5341.592773        0.144900            0.014448    5341.592773      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
4    layers.3.conv  weight  0.314412    73728   (128, 64, 3, 3)  ...   11132.814453        0.150998            0.015261   11132.814453      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
6    layers.4.conv  weight  0.291992   147456  (128, 128, 3, 3)  ...   20848.654297        0.141389            0.013431   20848.654297      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
8    layers.6.conv  weight  0.294668   294912  (256, 128, 3, 3)  ...   41785.378906        0.141688            0.013859   41785.378906      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
10   layers.7.conv  weight  0.304431   589824  (256, 256, 3, 3)  ...   85015.125000        0.144136            0.013674   85015.125000      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
12   layers.8.conv  weight  0.290792   589824  (256, 256, 3, 3)  ...   82092.585938        0.139181            0.012897   82092.585938      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
14  layers.10.conv  weight  0.291559  1179648  (512, 256, 3, 3)  ...  165819.281250        0.140567            0.011844  165819.281250      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
16  layers.11.conv  weight  0.193945  2359296  (512, 512, 3, 3)  ...  264392.937500        0.112064            0.008583  264392.937500      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
18  layers.12.conv  weight  0.049811  2359296  (512, 512, 3, 3)  ...  138777.953125        0.058822            0.003802  138777.953125      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
20  layers.14.conv  weight  0.007533  2359296  (512, 512, 3, 3)  ...   74926.140625        0.031758            0.001230   74926.140625      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
22  layers.15.conv  weight  0.002735  2359296  (512, 512, 3, 3)  ...   59823.328125        0.025356            0.000711   59823.328125      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
24  layers.16.conv  weight  0.005005  2359296  (512, 512, 3, 3)  ...   70122.359375        0.029722            0.001017   70122.359375      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
26              fc  weight  0.107031     5120         (10, 512)  ...     430.445740        0.084071            0.008304     430.445740      True
27              fc    bias  1.000000       10             (10,)  ...       0.000000        0.000000            0.000000       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 1475793/14719818 (0.1003)
FLOP Sparsity: 72127507/313478154 (0.2301)
Testing time: 0.21082140199723653
remaining weights: 
0        923.999977
1         64.000000
2      10809.000000
3         64.000000
4      23181.000732
5        128.000000
6      43056.000000
7        128.000000
8      86901.002930
9        256.000000
10    179561.003906
11       256.000000
12    171516.005859
13       256.000000
14    343937.003906
15       512.000000
16    457573.007812
17       512.000000
18    117519.996094
19       512.000000
20     17771.999634
21       512.000000
22      6453.000000
23       512.000000
24     11808.000000
25       512.000000
26       547.999992
27        10.000000
dtype: float64
flop each layer: [946175.9765625, 65536.0, 11068416.0, 65536.0, 5934336.1875, 32768.0, 11022336.0, 32768.0, 5561664.1875, 16384.0, 11491904.25, 16384.0, 10977024.375, 16384.0, 5502992.0625, 8192.0, 7321168.125, 8192.0, 1880319.9375, 8192.0, 71087.99853515625, 2048.0, 25812.0, 2048.0, 47232.0, 2048.0, 547.9999923706055, 10.0]
compression ratio: 2.0 ::: pruner: magTrain results:
                 train_loss  test_loss  top1_accuracy  top5_accuracy
Init.      0           NaN   2.417717          11.73          50.17
Pre-Prune  200    0.397699   0.572066          86.12          98.46
Post-Prune 0           NaN   2.833876          10.00          50.00
Final      200    2.009210   2.064085          19.10          59.19
Prune results:
             module   param  sparsity     size             shape  ...      score sum  score abs mean  score abs variance  score abs sum  prunable
0    layers.0.conv  weight  0.186921     1728     (64, 3, 3, 3)  ...     403.510468        0.233513            0.030640     403.510468      True
1    layers.0.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
2    layers.1.conv  weight  0.047445    36864    (64, 64, 3, 3)  ...    5341.592773        0.144900            0.014448    5341.592773      True
3    layers.1.conv    bias  1.000000       64             (64,)  ...       0.000000        0.000000            0.000000       0.000000     False
4    layers.3.conv  weight  0.052002    73728   (128, 64, 3, 3)  ...   11132.814453        0.150998            0.015261   11132.814453      True
5    layers.3.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
6    layers.4.conv  weight  0.042175   147456  (128, 128, 3, 3)  ...   20848.654297        0.141389            0.013431   20848.654297      True
7    layers.4.conv    bias  1.000000      128            (128,)  ...       0.000000        0.000000            0.000000       0.000000     False
8    layers.6.conv  weight  0.043959   294912  (256, 128, 3, 3)  ...   41785.378906        0.141688            0.013859   41785.378906      True
9    layers.6.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
10   layers.7.conv  weight  0.043620   589824  (256, 256, 3, 3)  ...   85015.125000        0.144136            0.013674   85015.125000      True
11   layers.7.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
12   layers.8.conv  weight  0.037584   589824  (256, 256, 3, 3)  ...   82092.585938        0.139181            0.012897   82092.585938      True
13   layers.8.conv    bias  1.000000      256            (256,)  ...       0.000000        0.000000            0.000000       0.000000     False
14  layers.10.conv  weight  0.032600  1179648  (512, 256, 3, 3)  ...  165819.281250        0.140567            0.011844  165819.281250      True
15  layers.10.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
16  layers.11.conv  weight  0.013092  2359296  (512, 512, 3, 3)  ...  264392.937500        0.112064            0.008583  264392.937500      True
17  layers.11.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
18  layers.12.conv  weight  0.001753  2359296  (512, 512, 3, 3)  ...  138777.953125        0.058822            0.003802  138777.953125      True
19  layers.12.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
20  layers.14.conv  weight  0.000145  2359296  (512, 512, 3, 3)  ...   74926.140625        0.031758            0.001230   74926.140625      True
21  layers.14.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
22  layers.15.conv  weight  0.000056  2359296  (512, 512, 3, 3)  ...   59823.328125        0.025356            0.000711   59823.328125      True
23  layers.15.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
24  layers.16.conv  weight  0.000055  2359296  (512, 512, 3, 3)  ...   70122.359375        0.029722            0.001017   70122.359375      True
25  layers.16.conv    bias  1.000000      512            (512,)  ...       0.000000        0.000000            0.000000       0.000000     False
26              fc  weight  0.017773     5120         (10, 512)  ...     430.445740        0.084071            0.008304     430.445740      True
27              fc    bias  1.000000       10             (10,)  ...       0.000000        0.000000            0.000000       0.000000     False

[28 rows x 13 columns]
Parameter Sparsity: 151389/14719818 (0.0103)
FLOP Sparsity: 10044984/313478154 (0.0320)
Testing time: 0.21388900799502153
remaining weights: 
0       323.000004
1        64.000000
2      1748.999954
3        64.000000
4      3834.000000
5       128.000000
6      6219.000000
7       128.000000
8     12964.000122
9       256.000000
10    25728.000732
11      256.000000
12    22167.999756
13      256.000000
14    38455.998047
15      512.000000
16    30888.000000
17      512.000000
18     4135.000122
19      512.000000
20      341.000004
21      512.000000
22      130.999998
23      512.000000
24      128.999997
25      512.000000
26       91.000004
27       10.000000
dtype: float64
flop each layer: [330752.00390625, 65536.0, 1790975.953125, 65536.0, 981504.0, 32768.0, 1592064.0, 32768.0, 829696.0078125, 16384.0, 1646592.046875, 16384.0, 1418751.984375, 16384.0, 615295.96875, 8192.0, 494208.0, 8192.0, 66160.001953125, 8192.0, 1364.000015258789, 2048.0, 523.9999923706055, 2048.0, 515.9999885559082, 2048.0, 91.00000381469727, 10.0]
